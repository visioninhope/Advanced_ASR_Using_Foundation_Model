Namespace(seed=123, train_json='data/train_10h.json', val_json='data/devsubset.json', test_clean_json='data/test_clean.json', test_other_json='data/test_other.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=3, lr=0.0001, vocab='data/vocab.txt', use_fbank=False, model='wav2vec2', report_interval=50, num_epochs=10)
Total number of model parameters is 94394794
EPOCH 1:
  batch 50 loss: 8.591153297424317
  batch 100 loss: 8.261098566055297
  batch 150 loss: 7.875825500488281
  batch 200 loss: 7.511341705322265
  batch 250 loss: 7.26962833404541
  batch 300 loss: 6.921295719146729
  batch 350 loss: 6.416494665145874
  batch 400 loss: 6.2441870307922365
  batch 450 loss: 5.73138876914978
  batch 500 loss: 5.330731439590454
  batch 550 loss: 5.144739465713501
  batch 600 loss: 4.807932615280151
  batch 650 loss: 4.437989587783814
LOSS train 4.43799 valid 4.57300, valid WER 100.00%
EPOCH 2:
  batch 50 loss: 4.109617614746094
  batch 100 loss: 3.950483145713806
  batch 150 loss: 3.779607377052307
  batch 200 loss: 3.669097933769226
  batch 250 loss: 3.5372880363464354
  batch 300 loss: 3.4520223426818846
  batch 350 loss: 3.386759648323059
  batch 400 loss: 3.312709188461304
  batch 450 loss: 3.250526146888733
  batch 500 loss: 3.2500500011444093
  batch 550 loss: 3.1747442626953126
  batch 600 loss: 3.1218105268478396
  batch 650 loss: 3.1357960987091062
LOSS train 3.13580 valid 3.27712, valid WER 100.00%
EPOCH 3:
  batch 50 loss: 3.050658259391785
  batch 100 loss: 3.047269244194031
  batch 150 loss: 3.017357578277588
  batch 200 loss: 2.9670540142059325
  batch 250 loss: 2.9639455127716063
  batch 300 loss: 2.956037983894348
  batch 350 loss: 2.953074164390564
  batch 400 loss: 2.9555505084991456
  batch 450 loss: 2.8723043966293336
  batch 500 loss: 2.856176943778992
  batch 550 loss: 2.837627286911011
  batch 600 loss: 2.8165684604644774
  batch 650 loss: 2.782903904914856
LOSS train 2.78290 valid 2.91293, valid WER 100.00%
EPOCH 4:
  batch 50 loss: 2.125967824459076
  batch 100 loss: 1.3517263102531434
  batch 150 loss: 0.9898091983795166
  batch 200 loss: 0.800565423965454
  batch 250 loss: 0.7150873672962189
  batch 300 loss: 0.6349773472547531
  batch 350 loss: 0.5536144119501114
  batch 400 loss: 0.5437815457582473
  batch 450 loss: 0.5199204909801484
  batch 500 loss: 0.47596022486686707
  batch 550 loss: 0.44560121357440946
  batch 600 loss: 0.45748756647109984
  batch 650 loss: 0.4430871111154556
LOSS train 0.44309 valid 0.47345, valid WER 33.76%
EPOCH 5:
  batch 50 loss: 0.32452368319034575
  batch 100 loss: 0.31238792687654493
  batch 150 loss: 0.3276850464940071
  batch 200 loss: 0.30873225688934325
  batch 250 loss: 0.31238189280033113
  batch 300 loss: 0.2957854184508324
  batch 350 loss: 0.2889110085368156
  batch 400 loss: 0.26182083547115326
  batch 450 loss: 0.24657254695892333
  batch 500 loss: 0.2452678906917572
  batch 550 loss: 0.2393828758597374
  batch 600 loss: 0.25418908685445785
  batch 650 loss: 0.26486242592334747
LOSS train 0.26486 valid 0.36179, valid WER 23.35%
EPOCH 6:
  batch 50 loss: 0.19393916815519333
  batch 100 loss: 0.1766279911994934
  batch 150 loss: 0.19073087304830552
  batch 200 loss: 0.1653037413954735
  batch 250 loss: 0.17516812533140183
  batch 300 loss: 0.18708458110690118
  batch 350 loss: 0.1790643234550953
  batch 400 loss: 0.16257460430264473
  batch 450 loss: 0.18545009940862656
  batch 500 loss: 0.16735129773616791
  batch 550 loss: 0.1674095107614994
  batch 600 loss: 0.15082480505108833
  batch 650 loss: 0.16306764081120492
LOSS train 0.16307 valid 0.33225, valid WER 19.85%
EPOCH 7:
  batch 50 loss: 0.12804009027779104
  batch 100 loss: 0.11862673230469227
  batch 150 loss: 0.11715523265302182
  batch 200 loss: 0.12819815441966056
  batch 250 loss: 0.12160977452993393
  batch 300 loss: 0.12258466258645058
  batch 350 loss: 0.1181602992862463
  batch 400 loss: 0.12271636448800564
  batch 450 loss: 0.10681347258388996
  batch 500 loss: 0.11981963403522969
  batch 550 loss: 0.11111725233495236
  batch 600 loss: 0.11768226824700832
  batch 650 loss: 0.11356213241815567
LOSS train 0.11356 valid 0.34860, valid WER 18.16%
EPOCH 8:
  batch 50 loss: 0.08542867712676525
  batch 100 loss: 0.10203736744821072
  batch 150 loss: 0.10139792531728745
  batch 200 loss: 0.09475400600582361
  batch 250 loss: 0.09360397160053253
  batch 300 loss: 0.09033304676413537
  batch 350 loss: 0.08838915415108203
  batch 400 loss: 0.09011669583618641
  batch 450 loss: 0.080664007589221
  batch 500 loss: 0.08685896962881089
  batch 550 loss: 0.08947560653090476
  batch 600 loss: 0.07859545577317477
  batch 650 loss: 0.10259418204426765
LOSS train 0.10259 valid 0.30933, valid WER 16.32%
EPOCH 9:
  batch 50 loss: 0.07192669454962015
  batch 100 loss: 0.06980470698326827
  batch 150 loss: 0.0653462378308177
  batch 200 loss: 0.06677084874361754
  batch 250 loss: 0.06805502690374851
  batch 300 loss: 0.07012972213327885
  batch 350 loss: 0.061370433270931245
  batch 400 loss: 0.06784740425646305
  batch 450 loss: 0.06637194722890855
  batch 500 loss: 0.07423014592379332
  batch 550 loss: 0.07083223070949315
  batch 600 loss: 0.07520623758435249
  batch 650 loss: 0.07344146048650145
LOSS train 0.07344 valid 0.32068, valid WER 16.26%
EPOCH 10:
  batch 50 loss: 0.06474060673266649
  batch 100 loss: 0.05470247644931078
  batch 150 loss: 0.049590458068996666
  batch 200 loss: 0.05665016829967499
  batch 250 loss: 0.05209332663565874
  batch 300 loss: 0.05263031244277954
  batch 350 loss: 0.05149929391220212
  batch 400 loss: 0.06422013137489557
  batch 450 loss: 0.05604563366621733
  batch 500 loss: 0.054821551702916624
  batch 550 loss: 0.05113019444048405
  batch 600 loss: 0.053851459976285695
  batch 650 loss: 0.05516686391085386
LOSS train 0.05517 valid 0.31303, valid WER 15.29%
Training finished in 25.0 minutes.
tensor([0.0731, 0.0732, 0.0725, 0.0714, 0.0721, 0.0708, 0.0703, 0.0711, 0.0738,
        0.0937, 0.1330, 0.1249], device='cuda:0', grad_fn=<SoftmaxBackward0>)
Model saved to checkpoints/20230308_191742/model_8
Loading model from checkpoints/20230308_191742/model_8
Results for test_clean:
SUB: 11.56%, DEL: 0.65%, INS: 0.82%, COR: 87.79%, WER: 13.03%
Results for test_other:
SUB: 19.45%, DEL: 1.68%, INS: 1.52%, COR: 78.87%, WER: 22.65%
