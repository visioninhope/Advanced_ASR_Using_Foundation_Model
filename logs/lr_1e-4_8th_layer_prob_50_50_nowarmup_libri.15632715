Namespace(seed=123, train_json='data/train_10h.json', val_json='data/devsubset.json', test_clean_json='data/test_clean.json', test_other_json='data/test_other.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=3, lr=0.0001, vocab='data/vocab.txt', use_fbank=False, model='wav2vec2', report_interval=50, num_epochs=10)
Total number of model parameters is 94394782
EPOCH 1:
  batch 50 loss: 8.443954076766968
  batch 100 loss: 8.075024518966675
  batch 150 loss: 7.658272285461425
  batch 200 loss: 7.26043025970459
  batch 250 loss: 7.003392496109009
  batch 300 loss: 6.637148027420044
  batch 350 loss: 6.165810737609863
  batch 400 loss: 5.985981769561768
  batch 450 loss: 5.465399522781372
  batch 500 loss: 5.088174695968628
  batch 550 loss: 4.91678560256958
  batch 600 loss: 4.620036082267761
  batch 650 loss: 4.311703085899353
LOSS train 4.31170 valid 4.98671, valid WER 99.96%
EPOCH 2:
  batch 50 loss: 4.054393630027771
  batch 100 loss: 3.927509722709656
  batch 150 loss: 3.744296140670776
  batch 200 loss: 3.6743613195419313
  batch 250 loss: 3.52968656539917
  batch 300 loss: 3.4733278512954713
  batch 350 loss: 3.390176272392273
  batch 400 loss: 3.3077741718292235
  batch 450 loss: 3.266263542175293
  batch 500 loss: 3.284474172592163
  batch 550 loss: 3.187464532852173
  batch 600 loss: 3.114154634475708
  batch 650 loss: 3.1395571994781495
LOSS train 3.13956 valid 3.78680, valid WER 100.00%
EPOCH 3:
  batch 50 loss: 3.0121219110488893
  batch 100 loss: 2.95645357131958
  batch 150 loss: 2.905817837715149
  batch 200 loss: 2.855774989128113
  batch 250 loss: 2.815341992378235
  batch 300 loss: 2.7821469497680664
  batch 350 loss: 2.7614609813690185
  batch 400 loss: 2.7299522447586058
  batch 450 loss: 2.603263282775879
  batch 500 loss: 2.590986156463623
  batch 550 loss: 2.5342077016830444
  batch 600 loss: 2.504768381118774
  batch 650 loss: 2.426720213890076
LOSS train 2.42672 valid 3.21204, valid WER 99.28%
EPOCH 4:
  batch 50 loss: 1.4466215324401857
  batch 100 loss: 0.984016728401184
  batch 150 loss: 0.8119876432418823
  batch 200 loss: 0.7062324070930481
  batch 250 loss: 0.6114911293983459
  batch 300 loss: 0.6046355783939361
  batch 350 loss: 0.5364124077558518
  batch 400 loss: 0.5451884585618972
  batch 450 loss: 0.5195178854465484
  batch 500 loss: 0.47444508790969847
  batch 550 loss: 0.4688196325302124
  batch 600 loss: 0.45040560245513916
  batch 650 loss: 0.4498957419395447
LOSS train 0.44990 valid 0.49809, valid WER 39.70%
EPOCH 5:
  batch 50 loss: 0.3446320816874504
  batch 100 loss: 0.31693812102079394
  batch 150 loss: 0.322119782269001
  batch 200 loss: 0.3092293557524681
  batch 250 loss: 0.3083968809247017
  batch 300 loss: 0.2851716846227646
  batch 350 loss: 0.3057619789242744
  batch 400 loss: 0.2766721701622009
  batch 450 loss: 0.27492217510938644
  batch 500 loss: 0.2711144459247589
  batch 550 loss: 0.26667201340198515
  batch 600 loss: 0.2773426806926727
  batch 650 loss: 0.2874482873082161
LOSS train 0.28745 valid 0.49177, valid WER 32.83%
EPOCH 6:
  batch 50 loss: 0.2240068259835243
  batch 100 loss: 0.21807412832975387
  batch 150 loss: 0.2107760502398014
  batch 200 loss: 0.20368542850017549
  batch 250 loss: 0.2056416876614094
  batch 300 loss: 0.21124657914042472
  batch 350 loss: 0.19757873192429543
  batch 400 loss: 0.207926404774189
  batch 450 loss: 0.2195540252327919
  batch 500 loss: 0.2055484376847744
  batch 550 loss: 0.19660251319408417
  batch 600 loss: 0.20309151247143745
  batch 650 loss: 0.20059102162718773
LOSS train 0.20059 valid 0.43573, valid WER 27.62%
EPOCH 7:
  batch 50 loss: 0.15848273038864136
  batch 100 loss: 0.1479709766805172
  batch 150 loss: 0.15725288048386574
  batch 200 loss: 0.15465540945529938
  batch 250 loss: 0.15523573517799377
  batch 300 loss: 0.1489369912445545
  batch 350 loss: 0.15099135249853135
  batch 400 loss: 0.1487089501321316
  batch 450 loss: 0.1446866862475872
  batch 500 loss: 0.141120732575655
  batch 550 loss: 0.14572923973202706
  batch 600 loss: 0.1451633830368519
  batch 650 loss: 0.14149466514587403
LOSS train 0.14149 valid 0.42628, valid WER 25.78%
EPOCH 8:
  batch 50 loss: 0.11126665309071541
  batch 100 loss: 0.11945318453013896
  batch 150 loss: 0.11621900901198387
  batch 200 loss: 0.10879296228289605
  batch 250 loss: 0.11607304640114308
  batch 300 loss: 0.11584186717867852
  batch 350 loss: 0.1116812263429165
  batch 400 loss: 0.11421992942690849
  batch 450 loss: 0.11492209121584893
  batch 500 loss: 0.11418520286679268
  batch 550 loss: 0.11344161182641983
  batch 600 loss: 0.11663322322070599
  batch 650 loss: 0.11990787580609322
LOSS train 0.11991 valid 0.42016, valid WER 24.94%
EPOCH 9:
  batch 50 loss: 0.09938972100615501
  batch 100 loss: 0.09932830169796944
  batch 150 loss: 0.08686240084469318
  batch 200 loss: 0.09188859678804874
  batch 250 loss: 0.09516997464001178
  batch 300 loss: 0.09126356571912765
  batch 350 loss: 0.09108135782182217
  batch 400 loss: 0.09277486115694046
  batch 450 loss: 0.0953302926570177
  batch 500 loss: 0.09126323282718658
  batch 550 loss: 0.09529438331723213
  batch 600 loss: 0.10943858444690704
  batch 650 loss: 0.09763308569788932
LOSS train 0.09763 valid 0.40653, valid WER 24.16%
EPOCH 10:
  batch 50 loss: 0.08961522698402405
  batch 100 loss: 0.07937687978148461
  batch 150 loss: 0.0802351239323616
  batch 200 loss: 0.07766138091683387
  batch 250 loss: 0.08983185589313507
  batch 300 loss: 0.08868162699043751
  batch 350 loss: 0.08427092365920544
  batch 400 loss: 0.08482002697885037
  batch 450 loss: 0.07493762269616128
  batch 500 loss: 0.07408719874918461
  batch 550 loss: 0.08212700966745615
  batch 600 loss: 0.08619043327867985
  batch 650 loss: 0.08593456327915192
LOSS train 0.08593 valid 0.40991, valid WER 23.39%
Training finished in 21.0 minutes.
Model saved to checkpoints/20230306_142201/model_9
Loading model from checkpoints/20230306_142201/model_9
Results for test_clean:
SUB: 16.16%, DEL: 0.91%, INS: 1.11%, COR: 82.94%, WER: 18.18%
Results for test_other:
SUB: 26.37%, DEL: 2.41%, INS: 1.96%, COR: 71.22%, WER: 30.75%
