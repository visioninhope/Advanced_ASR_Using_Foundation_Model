Namespace(seed=123, train_json='data/train_10h.json', val_json='data/devsubset.json', test_clean_json='data/test_clean.json', test_other_json='data/test_other.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=3, lr=0.0001, vocab='data/vocab.txt', use_fbank=False, model='wav2vec2', report_interval=50, num_epochs=10)
Total number of model parameters is 94394782
EPOCH 1:
  batch 50 loss: 8.535634050369262
  batch 100 loss: 8.064729557037353
  batch 150 loss: 7.520229530334473
  batch 200 loss: 6.99246542930603
  batch 250 loss: 6.602916536331176
  batch 300 loss: 6.105041580200195
  batch 350 loss: 5.580178241729737
  batch 400 loss: 5.274901084899902
  batch 450 loss: 4.786151571273804
  batch 500 loss: 4.3958361721038814
  batch 550 loss: 4.258385887145996
  batch 600 loss: 3.991052851676941
  batch 650 loss: 3.722278800010681
LOSS train 3.72228 valid 4.06348, valid WER 100.00%
EPOCH 2:
  batch 50 loss: 3.61757287979126
  batch 100 loss: 3.518708987236023
  batch 150 loss: 3.4087413930892945
  batch 200 loss: 3.355906481742859
  batch 250 loss: 3.29249276638031
  batch 300 loss: 3.232359519004822
  batch 350 loss: 3.219952096939087
  batch 400 loss: 3.1569955205917357
  batch 450 loss: 3.1145170736312866
  batch 500 loss: 3.1649992942810057
  batch 550 loss: 3.087004165649414
  batch 600 loss: 3.037658462524414
  batch 650 loss: 3.0700557231903076
LOSS train 3.07006 valid 3.29011, valid WER 100.00%
EPOCH 3:
  batch 50 loss: 2.9920207548141478
  batch 100 loss: 2.94827778339386
  batch 150 loss: 2.91086058139801
  batch 200 loss: 2.86316641330719
  batch 250 loss: 2.8489257097244263
  batch 300 loss: 2.8295058727264406
  batch 350 loss: 2.8354343795776367
  batch 400 loss: 2.798839530944824
  batch 450 loss: 2.7074492597579956
  batch 500 loss: 2.67364857673645
  batch 550 loss: 2.646575527191162
  batch 600 loss: 2.6311338138580322
  batch 650 loss: 2.5624972581863403
LOSS train 2.56250 valid 2.77970, valid WER 100.02%
EPOCH 4:
  batch 50 loss: 1.5602185177803038
  batch 100 loss: 0.9130358171463012
  batch 150 loss: 0.6940198576450348
  batch 200 loss: 0.5715393203496933
  batch 250 loss: 0.5744139802455902
  batch 300 loss: 0.5519435697793961
  batch 350 loss: 0.42112830460071565
  batch 400 loss: 0.3998237460851669
  batch 450 loss: 0.39247974812984465
  batch 500 loss: 0.3369905799627304
  batch 550 loss: 0.32715471982955935
  batch 600 loss: 0.32176159888505934
  batch 650 loss: 0.33275046646595
LOSS train 0.33275 valid 0.40607, valid WER 29.99%
EPOCH 5:
  batch 50 loss: 0.2448802450299263
  batch 100 loss: 0.2534882524609566
  batch 150 loss: 0.24154455095529556
  batch 200 loss: 0.21802343487739562
  batch 250 loss: 0.21189806401729583
  batch 300 loss: 0.2036753314733505
  batch 350 loss: 0.2218710446357727
  batch 400 loss: 0.20553798362612724
  batch 450 loss: 0.19215758085250856
  batch 500 loss: 0.18612989395856858
  batch 550 loss: 0.17950746908783913
  batch 600 loss: 0.23192387387156488
  batch 650 loss: 0.26197157353162764
LOSS train 0.26197 valid 0.37420, valid WER 23.50%
EPOCH 6:
  batch 50 loss: 0.17608915910124778
  batch 100 loss: 0.14286665126681328
  batch 150 loss: 0.14479259341955186
  batch 200 loss: 0.13476469814777375
  batch 250 loss: 0.14527854606509208
  batch 300 loss: 0.15581138879060746
  batch 350 loss: 0.13966153010725976
  batch 400 loss: 0.13953272096812724
  batch 450 loss: 0.13698567405343057
  batch 500 loss: 0.14532901175320148
  batch 550 loss: 0.13161344654858112
  batch 600 loss: 0.13636671751737595
  batch 650 loss: 0.13062818497419357
LOSS train 0.13063 valid 0.33830, valid WER 19.52%
EPOCH 7:
  batch 50 loss: 0.1031746968626976
  batch 100 loss: 0.09779009006917477
  batch 150 loss: 0.10195347405970097
  batch 200 loss: 0.12080452263355256
  batch 250 loss: 0.11131091497838497
  batch 300 loss: 0.09083277113735676
  batch 350 loss: 0.0888395357131958
  batch 400 loss: 0.10324211329221726
  batch 450 loss: 0.09509124897420407
  batch 500 loss: 0.09490921899676323
  batch 550 loss: 0.08830799736082554
  batch 600 loss: 0.0942729028314352
  batch 650 loss: 0.09386740885674953
LOSS train 0.09387 valid 0.32925, valid WER 17.11%
EPOCH 8:
  batch 50 loss: 0.06956864308565855
  batch 100 loss: 0.07418091550469398
  batch 150 loss: 0.06977632846683264
  batch 200 loss: 0.0703545767441392
  batch 250 loss: 0.06892924919724465
  batch 300 loss: 0.06968728311359883
  batch 350 loss: 0.08071224432438612
  batch 400 loss: 0.07121289644390344
  batch 450 loss: 0.06640782061964273
  batch 500 loss: 0.06679416295140982
  batch 550 loss: 0.07378777835518122
  batch 600 loss: 0.06985983181744813
  batch 650 loss: 0.07625680014491082
LOSS train 0.07626 valid 0.31499, valid WER 16.10%
EPOCH 9:
  batch 50 loss: 0.05522440375760198
  batch 100 loss: 0.06099736254662275
  batch 150 loss: 0.050185307804495094
  batch 200 loss: 0.05519911073148251
  batch 250 loss: 0.05960822220891714
  batch 300 loss: 0.050917087998241184
  batch 350 loss: 0.052741543762385844
  batch 400 loss: 0.05503782950341701
  batch 450 loss: 0.06094557028263807
  batch 500 loss: 0.05857808563858271
  batch 550 loss: 0.0580640447512269
  batch 600 loss: 0.056608315669000146
  batch 650 loss: 0.06567694179713726
LOSS train 0.06568 valid 0.32257, valid WER 15.68%
EPOCH 10:
  batch 50 loss: 0.05520030487328768
  batch 100 loss: 0.05398625357076526
  batch 150 loss: 0.04315915588289499
  batch 200 loss: 0.04527965595945716
  batch 250 loss: 0.05144545191898942
  batch 300 loss: 0.04730656504631042
  batch 350 loss: 0.047929729279130695
  batch 400 loss: 0.045235883351415394
  batch 450 loss: 0.03969917010515928
  batch 500 loss: 0.04058275327086449
  batch 550 loss: 0.04759488485753536
  batch 600 loss: 0.04919667430222034
  batch 650 loss: 0.04893074402585626
LOSS train 0.04893 valid 0.33687, valid WER 15.17%
Training finished in 24.0 minutes.
Model saved to checkpoints/20230306_192259/model_8
Loading model from checkpoints/20230306_192259/model_8
Results for test_clean:
SUB: 11.46%, DEL: 0.51%, INS: 0.79%, COR: 88.03%, WER: 12.75%
Results for test_other:
SUB: 19.34%, DEL: 1.64%, INS: 1.47%, COR: 79.01%, WER: 22.45%
