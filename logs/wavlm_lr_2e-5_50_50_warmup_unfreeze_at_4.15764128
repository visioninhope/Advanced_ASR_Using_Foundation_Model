Namespace(seed=123, train_json='data/train_10h.json', val_json='data/devsubset.json', test_clean_json='data/test_clean.json', test_other_json='data/test_other.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=3, lr=2e-05, vocab='data/vocab.txt', use_fbank=False, model='wavlm', report_interval=50, num_epochs=10)
Total number of model parameters is 94405006
EPOCH 1:
  batch 50 loss: 8.737809867858887
  batch 100 loss: 8.73544397354126
  batch 150 loss: 8.702073183059692
  batch 200 loss: 8.6722008228302
  batch 250 loss: 8.803749675750732
  batch 300 loss: 8.828861951828003
  batch 350 loss: 8.574656944274903
  batch 400 loss: 8.793035430908203
  batch 450 loss: 8.468080892562867
  batch 500 loss: 8.299847135543823
  batch 550 loss: 8.362008123397827
  batch 600 loss: 8.213176956176758
  batch 650 loss: 7.892759981155396
LOSS train 7.89276 valid 8.39095, valid WER 132.44%
EPOCH 2:
  batch 50 loss: 7.699384317398072
  batch 100 loss: 7.85822922706604
  batch 150 loss: 7.428170042037964
  batch 200 loss: 7.402898445129394
  batch 250 loss: 7.161768674850464
  batch 300 loss: 7.249792528152466
  batch 350 loss: 6.879691886901855
  batch 400 loss: 6.721465511322021
  batch 450 loss: 6.735904865264892
  batch 500 loss: 6.681305599212647
  batch 550 loss: 6.506217937469483
  batch 600 loss: 6.384095973968506
  batch 650 loss: 6.374348134994507
LOSS train 6.37435 valid 6.27617, valid WER 100.12%
EPOCH 3:
  batch 50 loss: 5.848761930465698
  batch 100 loss: 5.733351440429687
  batch 150 loss: 5.659829015731812
  batch 200 loss: 5.55497709274292
  batch 250 loss: 5.308970041275025
  batch 300 loss: 5.187711086273193
  batch 350 loss: 5.480278539657593
  batch 400 loss: 5.433372898101807
  batch 450 loss: 4.942645769119263
  batch 500 loss: 4.856643781661988
  batch 550 loss: 4.824601564407349
  batch 600 loss: 4.972482495307922
  batch 650 loss: 4.448335604667664
LOSS train 4.44834 valid 4.66438, valid WER 100.00%
EPOCH 4:
  batch 50 loss: 4.305773859024048
  batch 100 loss: 4.441461410522461
  batch 150 loss: 4.42894326210022
  batch 200 loss: 4.470540599822998
  batch 250 loss: 4.136898407936096
  batch 300 loss: 4.532950711250305
  batch 350 loss: 4.186997218132019
  batch 400 loss: 3.872015357017517
  batch 450 loss: 4.15198748588562
  batch 500 loss: 3.9712075901031496
  batch 550 loss: 3.8191789197921753
  batch 600 loss: 3.991518664360046
  batch 650 loss: 3.9272627639770508
LOSS train 3.92726 valid 3.78356, valid WER 100.00%
EPOCH 5:
  batch 50 loss: 3.1959080410003664
  batch 100 loss: 3.10965425491333
  batch 150 loss: 2.875308699607849
  batch 200 loss: 2.7438556337356568
  batch 250 loss: 2.512793469429016
  batch 300 loss: 2.3339141178131104
  batch 350 loss: 2.1035644960403443
  batch 400 loss: 1.8951175689697266
  batch 450 loss: 1.7107584118843078
  batch 500 loss: 1.5507531023025514
  batch 550 loss: 1.3291939282417298
  batch 600 loss: 1.2728219604492188
  batch 650 loss: 1.1773528337478638
LOSS train 1.17735 valid 0.99269, valid WER 68.71%
EPOCH 6:
  batch 50 loss: 1.1292882370948791
  batch 100 loss: 0.9215793776512146
  batch 150 loss: 0.9382701349258423
  batch 200 loss: 0.8962178993225097
  batch 250 loss: 0.8432515239715577
  batch 300 loss: 0.8504425358772277
  batch 350 loss: 0.6994150018692017
  batch 400 loss: 0.6743139600753785
  batch 450 loss: 0.7422279262542725
  batch 500 loss: 0.7702996343374252
  batch 550 loss: 0.7925353634357453
  batch 600 loss: 0.7151412522792816
  batch 650 loss: 0.6372015208005906
LOSS train 0.63720 valid 0.58891, valid WER 44.17%
EPOCH 7:
  batch 50 loss: 0.6177464902400971
  batch 100 loss: 0.5980337023735046
  batch 150 loss: 0.6393468952178956
  batch 200 loss: 0.5806668031215668
  batch 250 loss: 0.6284298950433731
  batch 300 loss: 0.5199414217472076
  batch 350 loss: 0.46667866349220277
  batch 400 loss: 0.5838717406988144
  batch 450 loss: 0.6515458661317826
  batch 500 loss: 0.5049620980024337
  batch 550 loss: 0.5033430683612824
  batch 600 loss: 0.4991141939163208
  batch 650 loss: 0.4242474412918091
LOSS train 0.42425 valid 0.45299, valid WER 36.77%
EPOCH 8:
  batch 50 loss: 0.3811874634027481
  batch 100 loss: 0.4992128723859787
  batch 150 loss: 0.4388177460432053
  batch 200 loss: 0.49007466226816176
  batch 250 loss: 0.4994115149974823
  batch 300 loss: 0.34665519803762435
  batch 350 loss: 0.39481545001268387
  batch 400 loss: 0.41624578952789304
  batch 450 loss: 0.4779611927270889
  batch 500 loss: 0.49637860625982283
  batch 550 loss: 0.437998765707016
  batch 600 loss: 0.34525731205940247
  batch 650 loss: 0.4411250275373459
LOSS train 0.44113 valid 0.39778, valid WER 32.67%
EPOCH 9:
  batch 50 loss: 0.49952411234378813
  batch 100 loss: 0.35399900943040846
  batch 150 loss: 0.3733059906959534
  batch 200 loss: 0.3579610276222229
  batch 250 loss: 0.39475914001464846
  batch 300 loss: 0.32382370173931124
  batch 350 loss: 0.3502054101228714
  batch 400 loss: 0.28105818420648576
  batch 450 loss: 0.31291584283113477
  batch 500 loss: 0.3471521106362343
  batch 550 loss: 0.4403437882661819
  batch 600 loss: 0.4621730875968933
  batch 650 loss: 0.32090045034885406
LOSS train 0.32090 valid 0.37942, valid WER 30.89%
EPOCH 10:
  batch 50 loss: 0.389312661588192
  batch 100 loss: 0.3599771711230278
  batch 150 loss: 0.3736428001523018
  batch 200 loss: 0.3150697445869446
  batch 250 loss: 0.33633112579584123
  batch 300 loss: 0.35340076208114624
  batch 350 loss: 0.2807228729128838
  batch 400 loss: 0.4134624195098877
  batch 450 loss: 0.4185509303212166
  batch 500 loss: 0.44771217197179797
  batch 550 loss: 0.36611599832773206
  batch 600 loss: 0.3509551066160202
  batch 650 loss: 0.5194900006055831
LOSS train 0.51949 valid 0.37939, valid WER 30.24%
Training finished in 20.0 minutes.
Model saved to checkpoints/20230308_224605/model_10
Loading model from checkpoints/20230308_224605/model_10
Results for test_clean:
SUB: 24.52%, DEL: 0.93%, INS: 0.95%, COR: 74.55%, WER: 26.40%
Results for test_other:
SUB: 29.83%, DEL: 2.05%, INS: 1.48%, COR: 68.13%, WER: 33.36%
