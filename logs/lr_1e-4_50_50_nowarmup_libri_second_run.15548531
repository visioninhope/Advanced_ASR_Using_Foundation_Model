Namespace(seed=123, train_json='data/train_10h.json', val_json='data/devsubset.json', test_clean_json='data/test_clean.json', test_other_json='data/test_other.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=3, lr=0.0001, vocab='data/vocab.txt', use_fbank=False, model='wav2vec2', report_interval=50, num_epochs=10)
Total number of model parameters is 94394782
EPOCH 1:
  batch 50 loss: 7.975290594100952
  batch 100 loss: 6.329888839721679
  batch 150 loss: 4.86619056224823
  batch 200 loss: 3.8783921813964843
  batch 250 loss: 3.4442296123504637
  batch 300 loss: 3.2184023809432984
  batch 350 loss: 3.1216023111343385
  batch 400 loss: 3.0812830781936644
  batch 450 loss: 3.006697487831116
  batch 500 loss: 2.9743788337707517
  batch 550 loss: 2.991347699165344
  batch 600 loss: 2.9601256608963014
  batch 650 loss: 2.9319717359542845
LOSS train 2.93197 valid 3.05329, valid WER 100.00%
EPOCH 2:
  batch 50 loss: 2.9397092866897583
  batch 100 loss: 2.9291448163986207
  batch 150 loss: 2.9190787506103515
  batch 200 loss: 2.89931414604187
  batch 250 loss: 2.8986685276031494
  batch 300 loss: 2.8973257303237916
  batch 350 loss: 2.8824768972396853
  batch 400 loss: 2.879025535583496
  batch 450 loss: 2.8767391443252563
  batch 500 loss: 2.875304899215698
  batch 550 loss: 2.8580631494522093
  batch 600 loss: 2.847036085128784
  batch 650 loss: 2.852684011459351
LOSS train 2.85268 valid 2.95795, valid WER 100.00%
EPOCH 3:
  batch 50 loss: 2.8267881155014036
  batch 100 loss: 2.811926283836365
  batch 150 loss: 2.797333626747131
  batch 200 loss: 2.7789772891998292
  batch 250 loss: 2.7877971267700197
  batch 300 loss: 2.776094298362732
  batch 350 loss: 2.7753312158584595
  batch 400 loss: 2.7524798202514646
  batch 450 loss: 2.706490077972412
  batch 500 loss: 2.71007426738739
  batch 550 loss: 2.69371874332428
  batch 600 loss: 2.669783992767334
  batch 650 loss: 2.654292097091675
LOSS train 2.65429 valid 2.77836, valid WER 99.98%
EPOCH 4:
  batch 50 loss: 2.0140819716453553
  batch 100 loss: 1.0155812764167786
  batch 150 loss: 0.690031898021698
  batch 200 loss: 0.5715786039829254
  batch 250 loss: 0.48232815325260164
  batch 300 loss: 0.43784632444381716
  batch 350 loss: 0.3925560283660889
  batch 400 loss: 0.3894358664751053
  batch 450 loss: 0.384948695898056
  batch 500 loss: 0.30406005173921585
  batch 550 loss: 0.3117884501814842
  batch 600 loss: 0.3605814528465271
  batch 650 loss: 0.34465236097574237
LOSS train 0.34465 valid 0.37416, valid WER 26.90%
EPOCH 5:
  batch 50 loss: 0.22766767144203187
  batch 100 loss: 0.23085205495357514
  batch 150 loss: 0.2132948260009289
  batch 200 loss: 0.22140084505081176
  batch 250 loss: 0.24210357084870338
  batch 300 loss: 0.21864413678646089
  batch 350 loss: 0.2230929321050644
  batch 400 loss: 0.21070927441120146
  batch 450 loss: 0.18332309111952783
  batch 500 loss: 0.17772434830665587
  batch 550 loss: 0.1965194098651409
  batch 600 loss: 0.22095831543207167
  batch 650 loss: 0.21620884761214257
LOSS train 0.21621 valid 0.35090, valid WER 21.15%
EPOCH 6:
  batch 50 loss: 0.15458422169089317
  batch 100 loss: 0.14703946605324744
  batch 150 loss: 0.13726191267371177
  batch 200 loss: 0.13689380064606665
  batch 250 loss: 0.14235395923256874
  batch 300 loss: 0.1577344162762165
  batch 350 loss: 0.14159516721963883
  batch 400 loss: 0.13874919444322587
  batch 450 loss: 0.1393683037161827
  batch 500 loss: 0.12963029764592648
  batch 550 loss: 0.1256827212870121
  batch 600 loss: 0.12419201403856278
  batch 650 loss: 0.12746472641825676
LOSS train 0.12746 valid 0.32907, valid WER 18.61%
EPOCH 7:
  batch 50 loss: 0.10947727046906948
  batch 100 loss: 0.10735792838037014
  batch 150 loss: 0.10499980233609676
  batch 200 loss: 0.11627823557704688
  batch 250 loss: 0.11438650496304036
  batch 300 loss: 0.10419531166553497
  batch 350 loss: 0.09846975356340408
  batch 400 loss: 0.10207493476569653
  batch 450 loss: 0.09794947668910027
  batch 500 loss: 0.10557812973856925
  batch 550 loss: 0.08777352295815945
  batch 600 loss: 0.10202681049704551
  batch 650 loss: 0.09171991527080536
LOSS train 0.09172 valid 0.29533, valid WER 16.76%
EPOCH 8:
  batch 50 loss: 0.07032141894102097
  batch 100 loss: 0.08266506474465132
  batch 150 loss: 0.07323888875544071
  batch 200 loss: 0.0737400620803237
  batch 250 loss: 0.0711957391910255
  batch 300 loss: 0.07642011359333992
  batch 350 loss: 0.07982912205159665
  batch 400 loss: 0.07915176123380661
  batch 450 loss: 0.07768843859434128
  batch 500 loss: 0.07689661966636777
  batch 550 loss: 0.0760498295351863
  batch 600 loss: 0.0663536761701107
  batch 650 loss: 0.0804820180684328
LOSS train 0.08048 valid 0.30009, valid WER 15.79%
EPOCH 9:
  batch 50 loss: 0.05771377334371209
  batch 100 loss: 0.05416663495823741
  batch 150 loss: 0.056449592523276804
  batch 200 loss: 0.05778437849134207
  batch 250 loss: 0.06032424533739686
  batch 300 loss: 0.0715309526398778
  batch 350 loss: 0.051692426428198816
  batch 400 loss: 0.05468388918787241
  batch 450 loss: 0.05427874058485031
  batch 500 loss: 0.060753528364002704
  batch 550 loss: 0.05327266555279493
  batch 600 loss: 0.0563115780428052
  batch 650 loss: 0.06343985114246607
LOSS train 0.06344 valid 0.29854, valid WER 15.15%
EPOCH 10:
  batch 50 loss: 0.04825684510171414
  batch 100 loss: 0.04867124330252409
  batch 150 loss: 0.04179527904838323
  batch 200 loss: 0.03924166105687618
  batch 250 loss: 0.04280475771054625
  batch 300 loss: 0.040849583074450496
  batch 350 loss: 0.05166158176958561
  batch 400 loss: 0.043770281309261916
  batch 450 loss: 0.04348925739526749
  batch 500 loss: 0.03929446835070848
  batch 550 loss: 0.042347175907343625
  batch 600 loss: 0.04131994749419391
  batch 650 loss: 0.04092823995277286
LOSS train 0.04093 valid 0.32004, valid WER 14.74%
Training finished in 24.0 minutes.
Model saved to checkpoints/20230305_203035/model_7
Loading model from checkpoints/20230305_203035/model_7
Results for test_clean:
SUB: 11.70%, DEL: 0.70%, INS: 0.77%, COR: 87.60%, WER: 13.16%
Results for test_other:
SUB: 19.32%, DEL: 1.87%, INS: 1.33%, COR: 78.81%, WER: 22.52%
