Namespace(seed=123, train_json='data/train_10h.json', val_json='data/devsubset.json', test_clean_json='data/test_clean.json', test_other_json='data/test_other.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=3, lr=0.0001, vocab='data/vocab.txt', use_fbank=False, model='wav2vec2', report_interval=50, num_epochs=10)
Total number of model parameters is 94678558
EPOCH 1:
  batch 50 loss: 6.020319023132324
  batch 100 loss: 3.12718355178833
  batch 150 loss: 2.9685649776458742
  batch 200 loss: 2.932264094352722
  batch 250 loss: 2.9151063680648805
  batch 300 loss: 2.8981330490112303
  batch 350 loss: 2.8948443937301636
  batch 400 loss: 2.8922817134857177
  batch 450 loss: 2.864376678466797
  batch 500 loss: 2.8625055360794067
  batch 550 loss: 2.865981912612915
  batch 600 loss: 2.858239517211914
  batch 650 loss: 2.849660611152649
LOSS train 2.84966 valid 2.98972, valid WER 100.00%
EPOCH 2:
  batch 50 loss: 2.8465667533874512
  batch 100 loss: 2.8308188199996946
  batch 150 loss: 2.8073010349273684
  batch 200 loss: 2.7838804435729982
  batch 250 loss: 2.7464989900588987
  batch 300 loss: 2.7090326166152954
  batch 350 loss: 2.653874173164368
  batch 400 loss: 2.592674732208252
  batch 450 loss: 2.5399966859817504
  batch 500 loss: 2.4802218294143676
  batch 550 loss: 2.422782850265503
  batch 600 loss: 2.359975481033325
  batch 650 loss: 2.323328013420105
LOSS train 2.32333 valid 2.50782, valid WER 99.48%
EPOCH 3:
  batch 50 loss: 2.257697467803955
  batch 100 loss: 2.225259575843811
  batch 150 loss: 2.1750740385055543
  batch 200 loss: 2.1524494600296022
  batch 250 loss: 2.1448728585243226
  batch 300 loss: 2.121396722793579
  batch 350 loss: 2.1182074546813965
  batch 400 loss: 2.1001734924316406
  batch 450 loss: 2.051303253173828
  batch 500 loss: 2.0613899779319764
  batch 550 loss: 2.0369300723075865
  batch 600 loss: 2.0295601367950438
  batch 650 loss: 2.0148112654685972
LOSS train 2.01481 valid 2.30499, valid WER 99.07%
EPOCH 4:
  batch 50 loss: 1.9917377209663392
  batch 100 loss: 1.9845092749595643
  batch 150 loss: 1.9736733269691467
  batch 200 loss: 1.967968692779541
  batch 250 loss: 1.964969882965088
  batch 300 loss: 1.9536376738548278
  batch 350 loss: 1.9390386581420898
  batch 400 loss: 1.9281439304351806
  batch 450 loss: 1.9268686985969543
  batch 500 loss: 1.9436998605728149
  batch 550 loss: 1.9124565839767456
  batch 600 loss: 1.9179295158386231
  batch 650 loss: 1.9102681922912597
LOSS train 1.91027 valid 2.23579, valid WER 98.97%
EPOCH 5:
  batch 50 loss: 1.873563861846924
  batch 100 loss: 1.8869755625724793
  batch 150 loss: 1.8672021436691284
  batch 200 loss: 1.8721033668518066
  batch 250 loss: 1.8794301891326903
  batch 300 loss: 1.8626714849472046
  batch 350 loss: 1.8755357336997986
  batch 400 loss: 1.8543664288520814
  batch 450 loss: 1.8486779117584229
  batch 500 loss: 1.8337629532814026
  batch 550 loss: 1.8502427005767823
  batch 600 loss: 1.8527190852165223
  batch 650 loss: 1.8451729488372803
LOSS train 1.84517 valid 2.19511, valid WER 98.95%
EPOCH 6:
  batch 50 loss: 1.8217824268341065
  batch 100 loss: 1.8093168330192566
  batch 150 loss: 1.824495358467102
  batch 200 loss: 1.8111839747428895
  batch 250 loss: 1.8266651034355164
  batch 300 loss: 1.823429629802704
  batch 350 loss: 1.803801851272583
  batch 400 loss: 1.7923842430114747
  batch 450 loss: 1.7921866011619567
  batch 500 loss: 1.8007539772987367
  batch 550 loss: 1.801454288959503
  batch 600 loss: 1.7988340139389039
  batch 650 loss: 1.7801573705673217
LOSS train 1.78016 valid 2.13846, valid WER 98.93%
EPOCH 7:
  batch 50 loss: 1.7873812246322631
  batch 100 loss: 1.787999997138977
  batch 150 loss: 1.7841985154151916
  batch 200 loss: 1.7804253911972046
  batch 250 loss: 1.7610695219039918
  batch 300 loss: 1.783180570602417
  batch 350 loss: 1.7670980143547057
  batch 400 loss: 1.7677822875976563
  batch 450 loss: 1.7655274033546449
  batch 500 loss: 1.7714831018447876
  batch 550 loss: 1.752040410041809
  batch 600 loss: 1.7622832107543944
  batch 650 loss: 1.7564398646354675
LOSS train 1.75644 valid 2.08972, valid WER 98.78%
EPOCH 8:
  batch 50 loss: 1.7367350792884826
  batch 100 loss: 1.7424285197257996
  batch 150 loss: 1.7568563413619995
  batch 200 loss: 1.7489817690849305
  batch 250 loss: 1.758640923500061
  batch 300 loss: 1.7568514156341553
  batch 350 loss: 1.7126145148277283
  batch 400 loss: 1.7170295739173889
  batch 450 loss: 1.7428815817832948
  batch 500 loss: 1.7312593269348144
  batch 550 loss: 1.7461662673950196
  batch 600 loss: 1.754698667526245
  batch 650 loss: 1.75212899684906
LOSS train 1.75213 valid 2.07175, valid WER 98.68%
EPOCH 9:
  batch 50 loss: 1.738755967617035
  batch 100 loss: 1.746122999191284
  batch 150 loss: 1.734008274078369
  batch 200 loss: 1.7270136427879335
  batch 250 loss: 1.7244382405281067
  batch 300 loss: 1.7341253304481505
  batch 350 loss: 1.7407646417617797
  batch 400 loss: 1.7383869767189026
  batch 450 loss: 1.7304461526870727
  batch 500 loss: 1.6980290555953979
  batch 550 loss: 1.7385008716583252
  batch 600 loss: 1.723359534740448
  batch 650 loss: 1.7495063686370849
LOSS train 1.74951 valid 2.06167, valid WER 98.66%
EPOCH 10:
  batch 50 loss: 1.7210276484489442
  batch 100 loss: 1.7330213975906372
  batch 150 loss: 1.697740342617035
  batch 200 loss: 1.7305846333503723
  batch 250 loss: 1.727736520767212
  batch 300 loss: 1.7108057236671448
  batch 350 loss: 1.727428982257843
  batch 400 loss: 1.7213594555854796
  batch 450 loss: 1.7196405553817748
  batch 500 loss: 1.7267266201972962
  batch 550 loss: 1.7066440868377686
  batch 600 loss: 1.7335952997207642
  batch 650 loss: 1.7380742740631103
LOSS train 1.73807 valid 2.05600, valid WER 98.68%
Training finished in 10.0 minutes.
Model saved to checkpoints/20230308_220945/model_10
Loading model from checkpoints/20230308_220945/model_10
Results for test_clean:
SUB: 49.32%, DEL: 49.16%, INS: 0.02%, COR: 1.53%, WER: 98.49%
Results for test_other:
SUB: 48.41%, DEL: 50.38%, INS: 0.04%, COR: 1.22%, WER: 98.82%
