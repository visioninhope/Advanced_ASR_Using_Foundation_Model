Namespace(seed=123, train_json='data/train_10h.json', val_json='data/devsubset.json', test_clean_json='data/test_clean.json', test_other_json='data/test_other.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=3, lr=0.0001, vocab='data/vocab.txt', use_fbank=False, model='wav2vec2', report_interval=50, num_epochs=10)
Total number of model parameters is 94394782
EPOCH 1:
  batch 50 loss: 7.9198177051544185
  batch 100 loss: 6.250319766998291
  batch 150 loss: 4.775580534934997
  batch 200 loss: 3.8519394397735596
  batch 250 loss: 3.440821161270142
  batch 300 loss: 3.2204318761825563
  batch 350 loss: 3.130166482925415
  batch 400 loss: 3.084021430015564
  batch 450 loss: 3.011614832878113
  batch 500 loss: 2.974413890838623
  batch 550 loss: 2.9914694261550903
  batch 600 loss: 2.9558765077590943
  batch 650 loss: 2.931128349304199
LOSS train 2.93113 valid 3.05970, valid WER 100.00%
EPOCH 2:
  batch 50 loss: 2.937327356338501
  batch 100 loss: 2.930909152030945
  batch 150 loss: 2.9182659339904786
  batch 200 loss: 2.9033067750930788
  batch 250 loss: 2.902421636581421
  batch 300 loss: 2.8974656772613527
  batch 350 loss: 2.8923407220840454
  batch 400 loss: 2.8848682546615603
  batch 450 loss: 2.8845750951766966
  batch 500 loss: 2.881106276512146
  batch 550 loss: 2.872209825515747
  batch 600 loss: 2.863493437767029
  batch 650 loss: 2.8619399452209473
LOSS train 2.86194 valid 2.98116, valid WER 100.00%
EPOCH 3:
  batch 50 loss: 2.8408553409576416
  batch 100 loss: 2.8319281005859374
  batch 150 loss: 2.8226650953292847
  batch 200 loss: 2.801934757232666
  batch 250 loss: 2.8108064222335813
  batch 300 loss: 2.795850229263306
  batch 350 loss: 2.7911633682250976
  batch 400 loss: 2.7757167434692382
  batch 450 loss: 2.739741678237915
  batch 500 loss: 2.7391768503189087
  batch 550 loss: 2.7207609939575197
  batch 600 loss: 2.7059994554519653
  batch 650 loss: 2.6932022953033448
LOSS train 2.69320 valid 2.81404, valid WER 100.00%
EPOCH 4:
  batch 50 loss: 2.1703032517433165
  batch 100 loss: 1.1220133566856385
  batch 150 loss: 0.7244610202312469
  batch 200 loss: 0.5523191982507706
  batch 250 loss: 0.4743895614147186
  batch 300 loss: 0.4427618932723999
  batch 350 loss: 0.37832782715559005
  batch 400 loss: 0.3685135743021965
  batch 450 loss: 0.3597163766622543
  batch 500 loss: 0.31160394579172135
  batch 550 loss: 0.30202137798070905
  batch 600 loss: 0.2897191420197487
  batch 650 loss: 0.28084114879369737
LOSS train 0.28084 valid 0.35649, valid WER 26.85%
EPOCH 5:
  batch 50 loss: 0.21278265804052354
  batch 100 loss: 0.19341468960046768
  batch 150 loss: 0.2024600176513195
  batch 200 loss: 0.19759926423430443
  batch 250 loss: 0.1920848758518696
  batch 300 loss: 0.18799126788973808
  batch 350 loss: 0.1860373142361641
  batch 400 loss: 0.1783241683244705
  batch 450 loss: 0.16060714304447174
  batch 500 loss: 0.16915227785706521
  batch 550 loss: 0.1518118353188038
  batch 600 loss: 0.1727970339357853
  batch 650 loss: 0.18507780626416206
LOSS train 0.18508 valid 0.30123, valid WER 19.58%
EPOCH 6:
  batch 50 loss: 0.1278321136534214
  batch 100 loss: 0.1251802258193493
  batch 150 loss: 0.12241345867514611
  batch 200 loss: 0.11295000322163105
  batch 250 loss: 0.1193313428759575
  batch 300 loss: 0.13077038571238517
  batch 350 loss: 0.12058625102043152
  batch 400 loss: 0.1154905004799366
  batch 450 loss: 0.11961961731314659
  batch 500 loss: 0.1217089194059372
  batch 550 loss: 0.11873515166342258
  batch 600 loss: 0.1105876686424017
  batch 650 loss: 0.1103454078733921
LOSS train 0.11035 valid 0.30231, valid WER 17.74%
EPOCH 7:
  batch 50 loss: 0.09764080040156842
  batch 100 loss: 0.0797570662945509
  batch 150 loss: 0.08181139539927244
  batch 200 loss: 0.0931626071035862
  batch 250 loss: 0.08593056105077267
  batch 300 loss: 0.08645333059132099
  batch 350 loss: 0.07887895330786705
  batch 400 loss: 0.08304431725293399
  batch 450 loss: 0.07587035819888115
  batch 500 loss: 0.0846391187980771
  batch 550 loss: 0.08182085819542408
  batch 600 loss: 0.08503258332610131
  batch 650 loss: 0.07422995433211327
LOSS train 0.07423 valid 0.27616, valid WER 14.86%
EPOCH 8:
  batch 50 loss: 0.06311052776873112
  batch 100 loss: 0.06426055230200291
  batch 150 loss: 0.06081783629953861
  batch 200 loss: 0.05716781280934811
  batch 250 loss: 0.06029105342924595
  batch 300 loss: 0.062488792799413204
  batch 350 loss: 0.06756144642829895
  batch 400 loss: 0.06653944112360477
  batch 450 loss: 0.05595916617661714
  batch 500 loss: 0.06126681875437498
  batch 550 loss: 0.06098961777985096
  batch 600 loss: 0.050004972256720064
  batch 650 loss: 0.06598557448014616
LOSS train 0.06599 valid 0.29777, valid WER 14.16%
EPOCH 9:
  batch 50 loss: 0.04753822723403573
  batch 100 loss: 0.05049521330744028
  batch 150 loss: 0.04668301410973072
  batch 200 loss: 0.052186215836554764
  batch 250 loss: 0.04924689959734678
  batch 300 loss: 0.050478856079280376
  batch 350 loss: 0.04184453731402755
  batch 400 loss: 0.04658518245443702
  batch 450 loss: 0.05598933957517147
  batch 500 loss: 0.050370457908138634
  batch 550 loss: 0.046359246987849476
  batch 600 loss: 0.05526884997263551
  batch 650 loss: 0.049863913608714935
LOSS train 0.04986 valid 0.31652, valid WER 13.73%
EPOCH 10:
  batch 50 loss: 0.0472915843129158
  batch 100 loss: 0.041645451225340364
  batch 150 loss: 0.03892524054273963
  batch 200 loss: 0.03688501935452223
  batch 250 loss: 0.044286645390093325
  batch 300 loss: 0.04287113690748811
  batch 350 loss: 0.03943677175790072
  batch 400 loss: 0.045641951570287345
  batch 450 loss: 0.0361641725897789
  batch 500 loss: 0.03606313267722726
  batch 550 loss: 0.042788720345124605
  batch 600 loss: 0.04571033388376236
  batch 650 loss: 0.04235933069139719
LOSS train 0.04236 valid 0.33259, valid WER 13.73%
Training finished in 22.0 minutes.
Model saved to checkpoints/20230307_144230/model_7
Loading model from checkpoints/20230307_144230/model_7
Results for test_clean:
SUB: 11.53%, DEL: 0.59%, INS: 0.83%, COR: 87.88%, WER: 12.95%
Results for test_other:
SUB: 17.28%, DEL: 1.38%, INS: 1.33%, COR: 81.34%, WER: 19.99%
