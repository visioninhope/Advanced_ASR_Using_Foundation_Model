Namespace(seed=123, train_json='data/train_10h.json', val_json='data/devsubset.json', test_clean_json='data/test_clean.json', test_other_json='data/test_other.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=3, lr=0.0001, vocab='data/vocab.txt', use_fbank=False, model='wav2vec2', report_interval=50, num_epochs=10)
Total number of model parameters is 94394782
EPOCH 1:
  batch 50 loss: 8.095280809402466
  batch 100 loss: 6.5056056308746335
  batch 150 loss: 4.974587478637695
  batch 200 loss: 3.9040360116958617
  batch 250 loss: 3.4234215927124025
  batch 300 loss: 3.2116762590408325
  batch 350 loss: 3.123038077354431
  batch 400 loss: 3.0754909753799438
  batch 450 loss: 3.007127056121826
  batch 500 loss: 2.9839399433135987
  batch 550 loss: 2.996013422012329
  batch 600 loss: 2.9614117908477784
  batch 650 loss: 2.9383624792099
LOSS train 2.93836 valid 3.05918, valid WER 100.00%
EPOCH 2:
  batch 50 loss: 2.940404291152954
  batch 100 loss: 2.9314531898498535
  batch 150 loss: 2.919457683563232
  batch 200 loss: 2.907697262763977
  batch 250 loss: 2.905365571975708
  batch 300 loss: 2.8985293197631834
  batch 350 loss: 2.8823867321014403
  batch 400 loss: 2.882520709037781
  batch 450 loss: 2.880969052314758
  batch 500 loss: 2.877336902618408
  batch 550 loss: 2.865600552558899
  batch 600 loss: 2.854906449317932
  batch 650 loss: 2.8622790575027466
LOSS train 2.86228 valid 2.96843, valid WER 100.00%
EPOCH 3:
  batch 50 loss: 2.833854579925537
  batch 100 loss: 2.8201203536987305
  batch 150 loss: 2.8084839534759523
  batch 200 loss: 2.788547167778015
  batch 250 loss: 2.7971123456954956
  batch 300 loss: 2.7823809385299683
  batch 350 loss: 2.7770651721954347
  batch 400 loss: 2.761702094078064
  batch 450 loss: 2.715497488975525
  batch 500 loss: 2.7172457218170165
  batch 550 loss: 2.7036424827575685
  batch 600 loss: 2.68636709690094
  batch 650 loss: 2.6646951484680175
LOSS train 2.66470 valid 2.79633, valid WER 99.98%
EPOCH 4:
  batch 50 loss: 2.02401832818985
  batch 100 loss: 1.0412025690078734
  batch 150 loss: 0.6842128384113312
  batch 200 loss: 0.5695002049207687
  batch 250 loss: 0.49071216583251953
  batch 300 loss: 0.6579023849964142
  batch 350 loss: 0.4372121697664261
  batch 400 loss: 0.41824459850788115
  batch 450 loss: 0.4055209809541702
  batch 500 loss: 0.34004198282957077
  batch 550 loss: 0.3462494820356369
  batch 600 loss: 0.3620324608683586
  batch 650 loss: 0.3408825075626373
LOSS train 0.34088 valid 0.38483, valid WER 29.08%
EPOCH 5:
  batch 50 loss: 0.24116178691387177
  batch 100 loss: 0.23466527581214905
  batch 150 loss: 0.24169637948274614
  batch 200 loss: 0.22749824762344362
  batch 250 loss: 0.23876794993877412
  batch 300 loss: 0.21840500891208647
  batch 350 loss: 0.20870709836483
  batch 400 loss: 0.20130762875080108
  batch 450 loss: 0.18382134422659874
  batch 500 loss: 0.18952032953500747
  batch 550 loss: 0.1773358428478241
  batch 600 loss: 0.2181415168941021
  batch 650 loss: 0.2344379998743534
LOSS train 0.23444 valid 0.41681, valid WER 24.46%
EPOCH 6:
  batch 50 loss: 0.1746828290820122
  batch 100 loss: 0.1515436862409115
  batch 150 loss: 0.154733384847641
  batch 200 loss: 0.13904033929109574
  batch 250 loss: 0.13534812286496162
  batch 300 loss: 0.15656743571162224
  batch 350 loss: 0.14580146878957748
  batch 400 loss: 0.15285467460751534
  batch 450 loss: 0.15521222814917565
  batch 500 loss: 0.15235667243599893
  batch 550 loss: 0.1509976367652416
  batch 600 loss: 0.1453936905413866
  batch 650 loss: 0.13483536556363107
LOSS train 0.13484 valid 0.32217, valid WER 18.78%
EPOCH 7:
  batch 50 loss: 0.11749599747359753
  batch 100 loss: 0.1065328261256218
  batch 150 loss: 0.10129354365170001
  batch 200 loss: 0.10735949903726577
  batch 250 loss: 0.10641069382429123
  batch 300 loss: 0.10134524412453175
  batch 350 loss: 0.09424543552100659
  batch 400 loss: 0.11092139154672623
  batch 450 loss: 0.10548364892601966
  batch 500 loss: 0.09668682090938091
  batch 550 loss: 0.08901826344430447
  batch 600 loss: 0.10314899414777756
  batch 650 loss: 0.09376197628676891
LOSS train 0.09376 valid 0.31468, valid WER 17.00%
EPOCH 8:
  batch 50 loss: 0.07575129017233849
  batch 100 loss: 0.08895319186151028
  batch 150 loss: 0.07461895078420638
  batch 200 loss: 0.08195185121148825
  batch 250 loss: 0.083392772115767
  batch 300 loss: 0.08169059559702874
  batch 350 loss: 0.07435400411486626
  batch 400 loss: 0.07563083499670029
  batch 450 loss: 0.07238558921962976
  batch 500 loss: 0.07373606100678444
  batch 550 loss: 0.07643266566097737
  batch 600 loss: 0.06812677040696144
  batch 650 loss: 0.08121746845543384
LOSS train 0.08122 valid 0.32957, valid WER 15.79%
EPOCH 9:
  batch 50 loss: 0.053624079078435895
  batch 100 loss: 0.058292672634124756
  batch 150 loss: 0.06308247242122889
  batch 200 loss: 0.060524115301668645
  batch 250 loss: 0.058315837606787685
  batch 300 loss: 0.06496972303837538
  batch 350 loss: 0.06904690697789193
  batch 400 loss: 0.06081940287724137
  batch 450 loss: 0.05629437355324626
  batch 500 loss: 0.05838398355990648
  batch 550 loss: 0.05813156921416521
  batch 600 loss: 0.05990235611796379
  batch 650 loss: 0.06478873670101165
LOSS train 0.06479 valid 0.34381, valid WER 15.68%
EPOCH 10:
  batch 50 loss: 0.05524606611579656
  batch 100 loss: 0.04782567884773016
  batch 150 loss: 0.043545424975454806
  batch 200 loss: 0.0449271029420197
  batch 250 loss: 0.04752833932638168
  batch 300 loss: 0.04795164337381721
  batch 350 loss: 0.045565961003303526
  batch 400 loss: 0.049440444614738226
  batch 450 loss: 0.043893294464796784
  batch 500 loss: 0.039980299724265936
  batch 550 loss: 0.050826768409460786
  batch 600 loss: 0.04691064160317183
  batch 650 loss: 0.04697552064433694
LOSS train 0.04698 valid 0.33015, valid WER 14.57%
Training finished in 24.0 minutes.
Model saved to checkpoints/20230305_170927/model_7
Loading model from checkpoints/20230305_170927/model_7
Results for test_clean:
SUB: 12.12%, DEL: 0.61%, INS: 0.82%, COR: 87.27%, WER: 13.55%
Results for test_other:
SUB: 19.39%, DEL: 1.74%, INS: 1.37%, COR: 78.87%, WER: 22.50%
