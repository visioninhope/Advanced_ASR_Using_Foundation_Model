Namespace(seed=123, train_json='data/train_10h.json', val_json='data/devsubset.json', test_clean_json='data/test_clean.json', test_other_json='data/test_other.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=3, lr=0.0001, vocab='data/vocab.txt', use_fbank=False, model='wav2vec2', report_interval=50, num_epochs=10)
Total number of model parameters is 94394782
EPOCH 1:
  batch 50 loss: 7.937459077835083
  batch 100 loss: 6.421871929168701
  batch 150 loss: 4.994185342788696
  batch 200 loss: 3.893937449455261
  batch 250 loss: 3.414147686958313
  batch 300 loss: 3.1988460874557494
  batch 350 loss: 3.102778515815735
  batch 400 loss: 3.058926863670349
  batch 450 loss: 2.991853551864624
  batch 500 loss: 2.9679378414154054
  batch 550 loss: 2.983204069137573
  batch 600 loss: 2.9536331701278686
  batch 650 loss: 2.9269831800460815
LOSS train 2.92698 valid 3.04926, valid WER 100.00%
EPOCH 2:
  batch 50 loss: 2.930810694694519
  batch 100 loss: 2.9197382068634035
  batch 150 loss: 2.9183003425598146
  batch 200 loss: 2.8946279191970827
  batch 250 loss: 2.89772741317749
  batch 300 loss: 2.8878119230270385
  batch 350 loss: 2.883831024169922
  batch 400 loss: 2.8758071279525756
  batch 450 loss: 2.8706329917907714
  batch 500 loss: 2.8649005365371703
  batch 550 loss: 2.8525827026367185
  batch 600 loss: 2.8397291326522827
  batch 650 loss: 2.8455643320083617
LOSS train 2.84556 valid 2.95311, valid WER 100.00%
EPOCH 3:
  batch 50 loss: 2.814381504058838
  batch 100 loss: 2.805632224082947
  batch 150 loss: 2.7890264225006103
  batch 200 loss: 2.7677563428878784
  batch 250 loss: 2.7770295095443727
  batch 300 loss: 2.7615771865844727
  batch 350 loss: 2.7577838563919066
  batch 400 loss: 2.746289668083191
  batch 450 loss: 2.6948082399368287
  batch 500 loss: 2.6932883882522582
  batch 550 loss: 2.6778194379806517
  batch 600 loss: 2.673177604675293
  batch 650 loss: 2.6497366762161256
LOSS train 2.64974 valid 2.77778, valid WER 99.96%
EPOCH 4:
  batch 50 loss: 2.03661408662796
  batch 100 loss: 1.0615933191776277
  batch 150 loss: 0.7602541315555572
  batch 200 loss: 0.5768141639232636
  batch 250 loss: 0.49668157994747164
  batch 300 loss: 0.45674035370349886
  batch 350 loss: 0.40216806054115295
  batch 400 loss: 0.4126681414246559
  batch 450 loss: 0.37815584540367125
  batch 500 loss: 0.37023885905742643
  batch 550 loss: 0.3451211816072464
  batch 600 loss: 0.3460653260350227
  batch 650 loss: 0.3389905655384064
LOSS train 0.33899 valid 0.39961, valid WER 28.50%
EPOCH 5:
  batch 50 loss: 0.22713708758354187
  batch 100 loss: 0.21816647708415984
  batch 150 loss: 0.22839527308940888
  batch 200 loss: 0.21102576851844787
  batch 250 loss: 0.22594006791710852
  batch 300 loss: 0.21197813287377357
  batch 350 loss: 0.21733553782105447
  batch 400 loss: 0.20789472803473472
  batch 450 loss: 0.18937272891402246
  batch 500 loss: 0.21238593861460686
  batch 550 loss: 0.19780513897538185
  batch 600 loss: 0.21463687732815742
  batch 650 loss: 0.21726692333817482
LOSS train 0.21727 valid 0.32518, valid WER 21.00%
EPOCH 6:
  batch 50 loss: 0.1607554965466261
  batch 100 loss: 0.13924627050757407
  batch 150 loss: 0.15185565784573554
  batch 200 loss: 0.15477382093667985
  batch 250 loss: 0.15811061576008797
  batch 300 loss: 0.1499474276602268
  batch 350 loss: 0.14788232892751693
  batch 400 loss: 0.14567411623895168
  batch 450 loss: 0.15490642562508583
  batch 500 loss: 0.143708129003644
  batch 550 loss: 0.14489426270127295
  batch 600 loss: 0.1317582333087921
  batch 650 loss: 0.12827434703707696
LOSS train 0.12827 valid 0.32921, valid WER 18.59%
EPOCH 7:
  batch 50 loss: 0.1091744413971901
  batch 100 loss: 0.09464687261730433
  batch 150 loss: 0.0907872373610735
  batch 200 loss: 0.0967877298593521
  batch 250 loss: 0.09688292428851128
  batch 300 loss: 0.09757241398096085
  batch 350 loss: 0.09761425919830799
  batch 400 loss: 0.10621784530580043
  batch 450 loss: 0.09695687599480152
  batch 500 loss: 0.08931157842278481
  batch 550 loss: 0.09412557050585747
  batch 600 loss: 0.09517130360007287
  batch 650 loss: 0.09060614012181759
LOSS train 0.09061 valid 0.36199, valid WER 16.10%
EPOCH 8:
  batch 50 loss: 0.06359130568802357
  batch 100 loss: 0.07728432074189186
  batch 150 loss: 0.07698625911027193
  batch 200 loss: 0.08037169706076383
  batch 250 loss: 0.07949934434145689
  batch 300 loss: 0.08161848619580268
  batch 350 loss: 0.08438520837575197
  batch 400 loss: 0.07143103498965502
  batch 450 loss: 0.07221578687429428
  batch 500 loss: 0.08217079021036625
  batch 550 loss: 0.07552195508033037
  batch 600 loss: 0.06230563312768936
  batch 650 loss: 0.0719349579885602
LOSS train 0.07193 valid 0.34333, valid WER 15.27%
EPOCH 9:
  batch 50 loss: 0.05431686105206609
  batch 100 loss: 0.06037220388650894
  batch 150 loss: 0.05439336454495788
  batch 200 loss: 0.07028200237080455
  batch 250 loss: 0.05841306732967496
  batch 300 loss: 0.0653700802847743
  batch 350 loss: 0.05854367189109325
  batch 400 loss: 0.05455743122845888
  batch 450 loss: 0.05682475481182337
  batch 500 loss: 0.058037194684147836
  batch 550 loss: 0.055022948710247876
  batch 600 loss: 0.055894335508346556
  batch 650 loss: 0.06013496823608875
LOSS train 0.06013 valid 0.37951, valid WER 14.86%
EPOCH 10:
  batch 50 loss: 0.05338262528181076
  batch 100 loss: 0.04738577900454402
  batch 150 loss: 0.04215136619284749
  batch 200 loss: 0.04144821893423796
  batch 250 loss: 0.05092203255742788
  batch 300 loss: 0.04465580139309168
  batch 350 loss: 0.04490770231932402
  batch 400 loss: 0.04764568654820323
  batch 450 loss: 0.0409035873785615
  batch 500 loss: 0.03823520611971617
  batch 550 loss: 0.04315774958580732
  batch 600 loss: 0.04543978575617075
  batch 650 loss: 0.05242718067020178
LOSS train 0.05243 valid 0.36861, valid WER 14.34%
Training finished in 24.0 minutes.
Model saved to checkpoints/20230305_195122/model_5
Loading model from checkpoints/20230305_195122/model_5
Results for test_clean:
SUB: 16.48%, DEL: 0.70%, INS: 0.93%, COR: 82.82%, WER: 18.11%
Results for test_other:
SUB: 23.24%, DEL: 1.96%, INS: 1.46%, COR: 74.80%, WER: 26.66%
