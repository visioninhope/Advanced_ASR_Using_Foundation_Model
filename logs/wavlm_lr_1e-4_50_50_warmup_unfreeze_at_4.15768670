Namespace(seed=123, train_json='data/train_10h.json', val_json='data/devsubset.json', test_clean_json='data/test_clean.json', test_other_json='data/test_other.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=3, lr=0.0001, vocab='data/vocab.txt', use_fbank=False, model='wavlm', report_interval=50, num_epochs=10)
Total number of model parameters is 94405006
EPOCH 1:
  batch 50 loss: 8.894030427932739
  batch 100 loss: 8.858117399215699
  batch 150 loss: 8.748131017684937
  batch 200 loss: 8.615123291015625
  batch 250 loss: 8.600081577301026
  batch 300 loss: 8.391545133590698
  batch 350 loss: 7.914157924652099
  batch 400 loss: 7.835054178237915
  batch 450 loss: 7.218562784194947
  batch 500 loss: 6.676293506622314
  batch 550 loss: 6.330978689193725
  batch 600 loss: 5.814746971130371
  batch 650 loss: 5.054482383728027
LOSS train 5.05448 valid 4.61250, valid WER 100.00%
EPOCH 2:
  batch 50 loss: 4.426311764717102
  batch 100 loss: 3.9395799684524535
  batch 150 loss: 3.645195460319519
  batch 200 loss: 3.9607343959808348
  batch 250 loss: 3.5942189788818357
  batch 300 loss: 3.493782625198364
  batch 350 loss: 3.3149743986129763
  batch 400 loss: 3.2570541858673097
  batch 450 loss: 3.770256199836731
  batch 500 loss: 3.2136388397216797
  batch 550 loss: 3.1084224224090575
  batch 600 loss: 3.443437204360962
  batch 650 loss: 2.98494402885437
LOSS train 2.98494 valid 2.96970, valid WER 100.00%
EPOCH 3:
  batch 50 loss: 2.852303509712219
  batch 100 loss: 2.9332387447357178
  batch 150 loss: 2.823705859184265
  batch 200 loss: 2.790537371635437
  batch 250 loss: 2.5551267051696778
  batch 300 loss: 2.6502678918838503
  batch 350 loss: 2.3449032497406006
  batch 400 loss: 2.42222261428833
  batch 450 loss: 2.6960547018051146
  batch 500 loss: 2.385018322467804
  batch 550 loss: 2.1488775396347046
  batch 600 loss: 2.0708583903312685
  batch 650 loss: 2.0270428442955017
LOSS train 2.02704 valid 1.97063, valid WER 99.53%
EPOCH 4:
  batch 50 loss: 2.073862452507019
  batch 100 loss: 1.7802798819541932
  batch 150 loss: 2.062654342651367
  batch 200 loss: 1.9995459938049316
  batch 250 loss: 1.6670826983451843
  batch 300 loss: 1.7048520231246949
  batch 350 loss: 1.7252511405944824
  batch 400 loss: 1.7185050821304322
  batch 450 loss: 1.7583015704154967
  batch 500 loss: 1.7698649978637695
  batch 550 loss: 1.4877846813201905
  batch 600 loss: 1.8151065278053284
  batch 650 loss: 1.7003126454353332
LOSS train 1.70031 valid 1.54037, valid WER 92.75%
EPOCH 5:
  batch 50 loss: 1.254271138906479
  batch 100 loss: 0.7666869509220123
  batch 150 loss: 0.6248810374736786
  batch 200 loss: 0.5030873429775238
  batch 250 loss: 0.5039282774925232
  batch 300 loss: 0.4906579905748367
  batch 350 loss: 0.5077473038434982
  batch 400 loss: 0.3747969707846642
  batch 450 loss: 0.37480711489915847
  batch 500 loss: 0.33008175492286684
  batch 550 loss: 0.3491155594587326
  batch 600 loss: 0.3511804932355881
  batch 650 loss: 0.3782327726483345
LOSS train 0.37823 valid 0.40350, valid WER 27.16%
EPOCH 6:
  batch 50 loss: 0.23650601506233215
  batch 100 loss: 0.2971421977877617
  batch 150 loss: 0.29386563286185263
  batch 200 loss: 0.19513359934091568
  batch 250 loss: 0.21465641722083093
  batch 300 loss: 0.2058803668618202
  batch 350 loss: 0.20501785919070245
  batch 400 loss: 0.2707426315546036
  batch 450 loss: 0.21883150011301042
  batch 500 loss: 0.22351106137037277
  batch 550 loss: 0.2063059589266777
  batch 600 loss: 0.2474324606359005
  batch 650 loss: 0.17468276098370553
LOSS train 0.17468 valid 0.28153, valid WER 20.45%
EPOCH 7:
  batch 50 loss: 0.15958457358181477
  batch 100 loss: 0.17741646245121956
  batch 150 loss: 0.14765453152358532
  batch 200 loss: 0.16142235539853572
  batch 250 loss: 0.22962388172745704
  batch 300 loss: 0.16066198207437993
  batch 350 loss: 0.11617371529340743
  batch 400 loss: 0.22924984097480774
  batch 450 loss: 0.1571921995282173
  batch 500 loss: 0.12507872946560383
  batch 550 loss: 0.15565800830721854
  batch 600 loss: 0.13046270713210106
  batch 650 loss: 0.1517199695855379
LOSS train 0.15172 valid 0.26825, valid WER 16.24%
EPOCH 8:
  batch 50 loss: 0.128744268193841
  batch 100 loss: 0.13318113125860692
  batch 150 loss: 0.15482246924191714
  batch 200 loss: 0.10448406852781772
  batch 250 loss: 0.11753323245793582
  batch 300 loss: 0.13857303760945797
  batch 350 loss: 0.12616020776331424
  batch 400 loss: 0.10278230633586645
  batch 450 loss: 0.1199396612495184
  batch 500 loss: 0.09061329104006291
  batch 550 loss: 0.10718717973679304
  batch 600 loss: 0.11382259421050549
  batch 650 loss: 0.10054919764399528
LOSS train 0.10055 valid 0.26974, valid WER 15.40%
EPOCH 9:
  batch 50 loss: 0.09391721487045288
  batch 100 loss: 0.10366875775158406
  batch 150 loss: 0.1195276265218854
  batch 200 loss: 0.08623854262754321
  batch 250 loss: 0.07640984192490578
  batch 300 loss: 0.14064519375562667
  batch 350 loss: 0.10499835848808288
  batch 400 loss: 0.07945143412798643
  batch 450 loss: 0.09608099598437547
  batch 500 loss: 0.08588646072894335
  batch 550 loss: 0.09664232965558767
  batch 600 loss: 0.09550761464983225
  batch 650 loss: 0.07685216087847949
LOSS train 0.07685 valid 0.25718, valid WER 14.24%
EPOCH 10:
  batch 50 loss: 0.08091902628540992
  batch 100 loss: 0.08412854596972466
  batch 150 loss: 0.09423349909484387
  batch 200 loss: 0.12390778888016939
  batch 250 loss: 0.11006549187004566
  batch 300 loss: 0.08129280902445317
  batch 350 loss: 0.06841363407671451
  batch 400 loss: 0.11982565466314554
  batch 450 loss: 0.061408917251974345
  batch 500 loss: 0.06110118389129639
  batch 550 loss: 0.059872878268361095
  batch 600 loss: 0.06794305082410573
  batch 650 loss: 0.08235816933214664
LOSS train 0.08236 valid 0.25860, valid WER 13.95%
Training finished in 20.0 minutes.
Total number of unfreezed  model parameters is 89809550
Model saved to checkpoints/20230309_003639/model_9
Loading model from checkpoints/20230309_003639/model_9
Results for test_clean:
SUB: 10.31%, DEL: 0.60%, INS: 0.63%, COR: 89.10%, WER: 11.53%
Results for test_other:
SUB: 16.42%, DEL: 1.54%, INS: 1.11%, COR: 82.04%, WER: 19.07%
