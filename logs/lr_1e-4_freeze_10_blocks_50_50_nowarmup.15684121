Namespace(seed=123, train_json='data/train_10h.json', val_json='data/devsubset.json', test_clean_json='data/test_clean.json', test_other_json='data/test_other.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=3, lr=0.0001, vocab='data/vocab.txt', use_fbank=False, model='wav2vec2', report_interval=50, num_epochs=10)
Total number of model parameters is 94394782
EPOCH 1:
  batch 50 loss: 7.940784931182861
  batch 100 loss: 6.290213975906372
  batch 150 loss: 4.755082211494446
  batch 200 loss: 3.8209038257598875
  batch 250 loss: 3.4162237787246705
  batch 300 loss: 3.2015673542022705
  batch 350 loss: 3.1290675735473634
  batch 400 loss: 3.0766790819168093
  batch 450 loss: 3.004849452972412
  batch 500 loss: 2.977834491729736
  batch 550 loss: 2.9894646072387694
  batch 600 loss: 2.9563841819763184
  batch 650 loss: 2.9285648488998413
LOSS train 2.92856 valid 3.06164, valid WER 100.00%
EPOCH 2:
  batch 50 loss: 2.934859528541565
  batch 100 loss: 2.9242914962768554
  batch 150 loss: 2.9197419834136964
  batch 200 loss: 2.902147455215454
  batch 250 loss: 2.901926808357239
  batch 300 loss: 2.8922256994247437
  batch 350 loss: 2.8902725267410276
  batch 400 loss: 2.883309121131897
  batch 450 loss: 2.8825373363494875
  batch 500 loss: 2.8794499254226684
  batch 550 loss: 2.871188230514526
  batch 600 loss: 2.854516386985779
  batch 650 loss: 2.8602924013137816
LOSS train 2.86029 valid 2.97757, valid WER 100.00%
EPOCH 3:
  batch 50 loss: 2.836018400192261
  batch 100 loss: 2.8311290836334226
  batch 150 loss: 2.816832227706909
  batch 200 loss: 2.7986829233169557
  batch 250 loss: 2.7994533491134646
  batch 300 loss: 2.7933559894561766
  batch 350 loss: 2.787747902870178
  batch 400 loss: 2.7720096254348756
  batch 450 loss: 2.7237564611434935
  batch 500 loss: 2.7304862785339354
  batch 550 loss: 2.71809720993042
  batch 600 loss: 2.700760507583618
  batch 650 loss: 2.6785472345352175
LOSS train 2.67855 valid 2.80739, valid WER 99.98%
EPOCH 4:
  batch 50 loss: 2.3594242334365845
  batch 100 loss: 1.8444354152679443
  batch 150 loss: 1.4771616530418397
  batch 200 loss: 1.2454180788993836
  batch 250 loss: 1.1104458916187285
  batch 300 loss: 0.9986703419685363
  batch 350 loss: 0.9277356874942779
  batch 400 loss: 0.8713575375080108
  batch 450 loss: 0.8284110724925995
  batch 500 loss: 0.7781686842441559
  batch 550 loss: 0.7582060408592224
  batch 600 loss: 0.7418026721477509
  batch 650 loss: 0.7046676468849182
LOSS train 0.70467 valid 0.77365, valid WER 63.31%
EPOCH 5:
  batch 50 loss: 0.6288175535202026
  batch 100 loss: 0.6114452087879181
  batch 150 loss: 0.6035914176702499
  batch 200 loss: 0.5802022016048431
  batch 250 loss: 0.5827088737487793
  batch 300 loss: 0.5578567069768906
  batch 350 loss: 0.5748907333612442
  batch 400 loss: 0.5625953978300094
  batch 450 loss: 0.5360327517986297
  batch 500 loss: 0.5197393333911896
  batch 550 loss: 0.5159071254730224
  batch 600 loss: 0.5245754861831665
  batch 650 loss: 0.5361445170640945
LOSS train 0.53614 valid 0.62333, valid WER 52.54%
EPOCH 6:
  batch 50 loss: 0.47603020787239075
  batch 100 loss: 0.45238976180553436
  batch 150 loss: 0.4585012304782867
  batch 200 loss: 0.44433166563510895
  batch 250 loss: 0.446507608294487
  batch 300 loss: 0.4482474786043167
  batch 350 loss: 0.4301452457904816
  batch 400 loss: 0.42780921995639803
  batch 450 loss: 0.4336629700660706
  batch 500 loss: 0.4326220881938934
  batch 550 loss: 0.4231180214881897
  batch 600 loss: 0.41638940513134004
  batch 650 loss: 0.4121552088856697
LOSS train 0.41216 valid 0.55891, valid WER 47.16%
EPOCH 7:
  batch 50 loss: 0.38498804092407224
  batch 100 loss: 0.38055854082107543
  batch 150 loss: 0.3731030344963074
  batch 200 loss: 0.3783276665210724
  batch 250 loss: 0.3766687762737274
  batch 300 loss: 0.3610814800858498
  batch 350 loss: 0.3708671671152115
  batch 400 loss: 0.3690795922279358
  batch 450 loss: 0.35437461018562316
  batch 500 loss: 0.35762916803359984
  batch 550 loss: 0.35750296860933306
  batch 600 loss: 0.3641157728433609
  batch 650 loss: 0.3512722390890122
LOSS train 0.35127 valid 0.51601, valid WER 43.59%
EPOCH 8:
  batch 50 loss: 0.3227247607707977
  batch 100 loss: 0.333724662065506
  batch 150 loss: 0.32678423523902894
  batch 200 loss: 0.32128496885299684
  batch 250 loss: 0.324836491048336
  batch 300 loss: 0.33517385452985765
  batch 350 loss: 0.32853919714689256
  batch 400 loss: 0.3228356197476387
  batch 450 loss: 0.3286085334420204
  batch 500 loss: 0.3130173721909523
  batch 550 loss: 0.3176235657930374
  batch 600 loss: 0.312708155810833
  batch 650 loss: 0.32716277331113813
LOSS train 0.32716 valid 0.50789, valid WER 41.72%
EPOCH 9:
  batch 50 loss: 0.2983469331264496
  batch 100 loss: 0.29268752932548525
  batch 150 loss: 0.29134239166975023
  batch 200 loss: 0.2869572067260742
  batch 250 loss: 0.2954808574914932
  batch 300 loss: 0.2943574583530426
  batch 350 loss: 0.29176578283309934
  batch 400 loss: 0.2889933329820633
  batch 450 loss: 0.28390265852212904
  batch 500 loss: 0.29047284185886385
  batch 550 loss: 0.2924613079428673
  batch 600 loss: 0.29714266330003736
  batch 650 loss: 0.297976698577404
LOSS train 0.29798 valid 0.50457, valid WER 40.15%
EPOCH 10:
  batch 50 loss: 0.28435501545667646
  batch 100 loss: 0.26814690202474595
  batch 150 loss: 0.2545197635889053
  batch 200 loss: 0.2642935338616371
  batch 250 loss: 0.2684247687458992
  batch 300 loss: 0.2613521033525467
  batch 350 loss: 0.26659114599227907
  batch 400 loss: 0.27712056040763855
  batch 450 loss: 0.274205079972744
  batch 500 loss: 0.26754635721445086
  batch 550 loss: 0.2717364627122879
  batch 600 loss: 0.2811886265873909
  batch 650 loss: 0.2747823867201805
LOSS train 0.27478 valid 0.50451, valid WER 40.07%
Training finished in 24.0 minutes.
Model saved to checkpoints/20230307_152532/model_10
Loading model from checkpoints/20230307_152532/model_10
Results for test_clean:
SUB: 32.68%, DEL: 1.67%, INS: 2.11%, COR: 65.65%, WER: 36.46%
Results for test_other:
SUB: 39.35%, DEL: 3.03%, INS: 2.38%, COR: 57.62%, WER: 44.76%
