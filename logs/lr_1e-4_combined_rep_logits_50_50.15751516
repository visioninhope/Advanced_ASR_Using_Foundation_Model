Namespace(seed=123, train_json='data/train_10h.json', val_json='data/devsubset.json', test_clean_json='data/test_clean.json', test_other_json='data/test_other.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=3, lr=0.0001, vocab='data/vocab.txt', use_fbank=False, model='wav2vec2', report_interval=50, num_epochs=10)
Total number of model parameters is 94394794
EPOCH 1:
  batch 50 loss: 7.24674222946167
  batch 100 loss: 5.459596385955811
  batch 150 loss: 4.306124620437622
  batch 200 loss: 3.487090630531311
  batch 250 loss: 3.128410587310791
  batch 300 loss: 2.8376614809036256
  batch 350 loss: 2.58011417388916
  batch 400 loss: 2.320065698623657
  batch 450 loss: 2.0479957675933838
  batch 500 loss: 1.8472108554840088
  batch 550 loss: 1.7233888483047486
  batch 600 loss: 1.6336912965774537
  batch 650 loss: 1.4999894547462462
LOSS train 1.49999 valid 1.49751, valid WER 93.22%
EPOCH 2:
  batch 50 loss: 1.3944105982780457
  batch 100 loss: 1.36306631565094
  batch 150 loss: 1.3072633671760558
  batch 200 loss: 1.2868272495269775
  batch 250 loss: 1.2274227952957153
  batch 300 loss: 1.1975100350379944
  batch 350 loss: 1.1607816767692567
  batch 400 loss: 1.1462678837776183
  batch 450 loss: 1.1128622269630433
  batch 500 loss: 1.1081355726718902
  batch 550 loss: 1.0956563067436218
  batch 600 loss: 1.0571705663204194
  batch 650 loss: 1.055238037109375
LOSS train 1.05524 valid 1.11992, valid WER 82.28%
EPOCH 3:
  batch 50 loss: 1.0223223400115966
  batch 100 loss: 0.9987625622749329
  batch 150 loss: 0.9879590833187103
  batch 200 loss: 0.9719819819927216
  batch 250 loss: 0.9668096661567688
  batch 300 loss: 0.9577873754501343
  batch 350 loss: 0.9398431110382081
  batch 400 loss: 0.9652361130714416
  batch 450 loss: 0.9072071671485901
  batch 500 loss: 0.9275991356372834
  batch 550 loss: 0.8922911775112152
  batch 600 loss: 0.9110142660140991
  batch 650 loss: 0.8919183635711669
LOSS train 0.89192 valid 0.99022, valid WER 76.46%
EPOCH 4:
  batch 50 loss: 0.7289653503894806
  batch 100 loss: 0.5610718822479248
  batch 150 loss: 0.5180088740587234
  batch 200 loss: 0.4292227154970169
  batch 250 loss: 0.4020832186937332
  batch 300 loss: 0.3925754004716873
  batch 350 loss: 0.34525855898857116
  batch 400 loss: 0.36431780248880385
  batch 450 loss: 0.34581367671489716
  batch 500 loss: 0.3070573490858078
  batch 550 loss: 0.3292180395126343
  batch 600 loss: 0.3275967741012573
  batch 650 loss: 0.32756985157728197
LOSS train 0.32757 valid 0.39107, valid WER 28.55%
EPOCH 5:
  batch 50 loss: 0.207670466452837
  batch 100 loss: 0.2006627680361271
  batch 150 loss: 0.1820939365029335
  batch 200 loss: 0.1917380441725254
  batch 250 loss: 0.19041601195931435
  batch 300 loss: 0.18372443288564683
  batch 350 loss: 0.17702216997742654
  batch 400 loss: 0.17108877629041672
  batch 450 loss: 0.1698951780796051
  batch 500 loss: 0.17068407267332078
  batch 550 loss: 0.16195108726620674
  batch 600 loss: 0.17628717720508574
  batch 650 loss: 0.18259374991059305
LOSS train 0.18259 valid 0.36265, valid WER 22.03%
EPOCH 6:
  batch 50 loss: 0.14034202709794044
  batch 100 loss: 0.1185902987420559
  batch 150 loss: 0.1331862659752369
  batch 200 loss: 0.11639067403972149
  batch 250 loss: 0.14969484731554986
  batch 300 loss: 0.12743653707206248
  batch 350 loss: 0.13169267669320106
  batch 400 loss: 0.1283269889652729
  batch 450 loss: 0.14087252736091613
  batch 500 loss: 0.12646373510360717
  batch 550 loss: 0.1157675464451313
  batch 600 loss: 0.11166765756905078
  batch 650 loss: 0.11677439995110035
LOSS train 0.11677 valid 0.33782, valid WER 18.41%
EPOCH 7:
  batch 50 loss: 0.10650992169976234
  batch 100 loss: 0.08302205182611942
  batch 150 loss: 0.09428108394145966
  batch 200 loss: 0.08798929907381535
  batch 250 loss: 0.08301737681031227
  batch 300 loss: 0.07897844985127449
  batch 350 loss: 0.09201338496059179
  batch 400 loss: 0.08581765301525593
  batch 450 loss: 0.08727517738938331
  batch 500 loss: 0.09502314187586308
  batch 550 loss: 0.08401672940701246
  batch 600 loss: 0.07864479262381792
  batch 650 loss: 0.11819295600056648
LOSS train 0.11819 valid 0.33782, valid WER 16.92%
EPOCH 8:
  batch 50 loss: 0.06398608542978763
  batch 100 loss: 0.08894075058400631
  batch 150 loss: 0.09371211264282465
  batch 200 loss: 0.07601372852921486
  batch 250 loss: 0.0664509081467986
  batch 300 loss: 0.06417241219431162
  batch 350 loss: 0.0704712089523673
  batch 400 loss: 0.07065776500850916
  batch 450 loss: 0.05888810932636261
  batch 500 loss: 0.06451311893761158
  batch 550 loss: 0.05783476870507002
  batch 600 loss: 0.054605572689324616
  batch 650 loss: 0.06575454197824002
LOSS train 0.06575 valid 0.31859, valid WER 15.44%
EPOCH 9:
  batch 50 loss: 0.04849121659994125
  batch 100 loss: 0.0511518126912415
  batch 150 loss: 0.04118319880217314
  batch 200 loss: 0.0574285969696939
  batch 250 loss: 0.054403547346591946
  batch 300 loss: 0.05906625157222152
  batch 350 loss: 0.051226822379976514
  batch 400 loss: 0.050016218200325965
  batch 450 loss: 0.04557631883770227
  batch 500 loss: 0.048159943632781504
  batch 550 loss: 0.05144676011055708
  batch 600 loss: 0.052731865476816894
  batch 650 loss: 0.05548400826752186
LOSS train 0.05548 valid 0.31065, valid WER 15.25%
EPOCH 10:
  batch 50 loss: 0.042255364432930945
  batch 100 loss: 0.03861866970546544
  batch 150 loss: 0.03473481554538012
  batch 200 loss: 0.04094269344583154
  batch 250 loss: 0.04036022922024131
  batch 300 loss: 0.042488608527928594
  batch 350 loss: 0.03796061461791396
  batch 400 loss: 0.043306593243032696
  batch 450 loss: 0.041452381825074554
  batch 500 loss: 0.035589628480374814
  batch 550 loss: 0.03853248338215053
  batch 600 loss: 0.04012457886710763
  batch 650 loss: 0.03737084208056331
LOSS train 0.03737 valid 0.33079, valid WER 14.55%
Training finished in 24.0 minutes.
Parameter containing:
tensor([0.8405, 0.8507, 0.9013, 0.9912, 1.0702, 1.1137, 1.1449, 1.1724, 1.2101,
        1.2146, 1.1252, 1.0942], device='cuda:0', requires_grad=True)
Model saved to checkpoints/20230308_184709/model_9
Loading model from checkpoints/20230308_184709/model_9
Results for test_clean:
SUB: 10.36%, DEL: 0.64%, INS: 0.70%, COR: 89.00%, WER: 11.70%
Results for test_other:
SUB: 17.60%, DEL: 1.45%, INS: 1.32%, COR: 80.95%, WER: 20.37%
