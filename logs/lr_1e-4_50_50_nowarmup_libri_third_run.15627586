Namespace(seed=123, train_json='data/train_10h.json', val_json='data/devsubset.json', test_clean_json='data/test_clean.json', test_other_json='data/test_other.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=3, lr=0.0001, vocab='data/vocab.txt', use_fbank=False, model='wav2vec2', report_interval=50, num_epochs=10)
Total number of model parameters is 94394782
EPOCH 1:
  batch 50 loss: 8.194739542007447
  batch 100 loss: 6.5917480945587155
  batch 150 loss: 4.993891186714173
  batch 200 loss: 3.920592532157898
  batch 250 loss: 3.43982213973999
  batch 300 loss: 3.215895471572876
  batch 350 loss: 3.128548183441162
  batch 400 loss: 3.0907961463928224
  batch 450 loss: 3.011127152442932
  batch 500 loss: 2.9850149726867676
  batch 550 loss: 3.00777060508728
  batch 600 loss: 2.9690061902999876
  batch 650 loss: 2.9397137498855592
LOSS train 2.93971 valid 3.06256, valid WER 99.98%
EPOCH 2:
  batch 50 loss: 2.9455549907684326
  batch 100 loss: 2.93782751083374
  batch 150 loss: 2.9284891176223753
  batch 200 loss: 2.9132989978790285
  batch 250 loss: 2.9122235870361326
  batch 300 loss: 2.9031599950790405
  batch 350 loss: 2.8978708457946776
  batch 400 loss: 2.8947602033615114
  batch 450 loss: 2.8917701864242553
  batch 500 loss: 2.8916120672225953
  batch 550 loss: 2.8798330783843995
  batch 600 loss: 2.8708631563186646
  batch 650 loss: 2.8810399055480955
LOSS train 2.88104 valid 2.99071, valid WER 100.00%
EPOCH 3:
  batch 50 loss: 2.8599696350097656
  batch 100 loss: 2.8522403860092163
  batch 150 loss: 2.8403359174728395
  batch 200 loss: 2.8223131322860717
  batch 250 loss: 2.8353776502609254
  batch 300 loss: 2.819890146255493
  batch 350 loss: 2.8231193017959595
  batch 400 loss: 2.8031164836883544
  batch 450 loss: 2.7714135885238647
  batch 500 loss: 2.7702018928527834
  batch 550 loss: 2.756581773757935
  batch 600 loss: 2.7444636821746826
  batch 650 loss: 2.7256843566894533
LOSS train 2.72568 valid 2.83929, valid WER 100.00%
EPOCH 4:
  batch 50 loss: 2.3175595593452454
  batch 100 loss: 1.3332688200473786
  batch 150 loss: 0.8609739124774933
  batch 200 loss: 0.6302773118019104
  batch 250 loss: 0.6855392825603485
  batch 300 loss: 0.5008200722932815
  batch 350 loss: 0.43622765958309173
  batch 400 loss: 0.46763736307621
  batch 450 loss: 0.45183811843395233
  batch 500 loss: 0.3768794161081314
  batch 550 loss: 0.35432814687490466
  batch 600 loss: 0.33374357134103777
  batch 650 loss: 0.36446891486644745
LOSS train 0.36447 valid 0.41713, valid WER 30.15%
EPOCH 5:
  batch 50 loss: 0.23810983940958977
  batch 100 loss: 0.23298611998558044
  batch 150 loss: 0.236795991063118
  batch 200 loss: 0.2141412740945816
  batch 250 loss: 0.2450789052248001
  batch 300 loss: 0.24358582466840745
  batch 350 loss: 0.22944069236516954
  batch 400 loss: 0.2227970039844513
  batch 450 loss: 0.21049590080976485
  batch 500 loss: 0.21053819984197616
  batch 550 loss: 0.2328328463435173
  batch 600 loss: 0.2237978196144104
  batch 650 loss: 0.23490202009677888
LOSS train 0.23490 valid 0.36799, valid WER 23.19%
EPOCH 6:
  batch 50 loss: 0.15788966573774815
  batch 100 loss: 0.14042689442634582
  batch 150 loss: 0.15408646449446678
  batch 200 loss: 0.15176442623138428
  batch 250 loss: 0.15110685378313066
  batch 300 loss: 0.17112634152173997
  batch 350 loss: 0.15254192993044854
  batch 400 loss: 0.14489292874932289
  batch 450 loss: 0.14759604938328266
  batch 500 loss: 0.19116807833313942
  batch 550 loss: 0.2027590610086918
  batch 600 loss: 0.17665088266134263
  batch 650 loss: 0.1465253061056137
LOSS train 0.14653 valid 0.33939, valid WER 18.61%
EPOCH 7:
  batch 50 loss: 0.1155705001950264
  batch 100 loss: 0.19756919026374817
  batch 150 loss: 0.12420860841870308
  batch 200 loss: 0.12587919130921363
  batch 250 loss: 0.15770155631005764
  batch 300 loss: 0.1518075679242611
  batch 350 loss: 0.11108464598655701
  batch 400 loss: 0.12199889279901982
  batch 450 loss: 0.10715465255081653
  batch 500 loss: 0.10416781611740589
  batch 550 loss: 0.10094754315912724
  batch 600 loss: 0.10080500341951847
  batch 650 loss: 0.11149692960083485
LOSS train 0.11150 valid 0.33087, valid WER 16.96%
EPOCH 8:
  batch 50 loss: 0.0707312511652708
  batch 100 loss: 0.087653477974236
  batch 150 loss: 0.08152258194983006
  batch 200 loss: 0.08678544513881206
  batch 250 loss: 0.07904927216470242
  batch 300 loss: 0.08502585276961326
  batch 350 loss: 0.0775274395942688
  batch 400 loss: 0.07947056859731674
  batch 450 loss: 0.07045098591595889
  batch 500 loss: 0.08583048656582833
  batch 550 loss: 0.0811148932389915
  batch 600 loss: 0.06725107608363032
  batch 650 loss: 0.08549234330654144
LOSS train 0.08549 valid 0.32757, valid WER 15.62%
EPOCH 9:
  batch 50 loss: 0.054224462155252694
  batch 100 loss: 0.06223300412297249
  batch 150 loss: 0.05625039592385292
  batch 200 loss: 0.06290522366762161
  batch 250 loss: 0.06314535547047853
  batch 300 loss: 0.05802644850686192
  batch 350 loss: 0.05342640019953251
  batch 400 loss: 0.057920937687158586
  batch 450 loss: 0.05805252749472856
  batch 500 loss: 0.05876759521663189
  batch 550 loss: 0.05181945111602545
  batch 600 loss: 0.06739597871899605
  batch 650 loss: 0.06346573084592819
LOSS train 0.06347 valid 0.30789, valid WER 15.27%
EPOCH 10:
  batch 50 loss: 0.054813488721847534
  batch 100 loss: 0.05180768229998648
  batch 150 loss: 0.03945588537491858
  batch 200 loss: 0.045850882809609174
  batch 250 loss: 0.04728555453941226
  batch 300 loss: 0.05599300775676966
  batch 350 loss: 0.046114235296845434
  batch 400 loss: 0.04652050215750933
  batch 450 loss: 0.05019881669431925
  batch 500 loss: 0.03885324591770768
  batch 550 loss: 0.04801619502715766
  batch 600 loss: 0.047286667469888924
  batch 650 loss: 0.049357416722923514
LOSS train 0.04936 valid 0.30838, valid WER 14.45%
Training finished in 29.0 minutes.
Model saved to checkpoints/20230306_130307/model_10
Loading model from checkpoints/20230306_130307/model_10
Results for test_clean:
SUB: 10.14%, DEL: 0.54%, INS: 0.69%, COR: 89.31%, WER: 11.38%
Results for test_other:
SUB: 17.51%, DEL: 1.60%, INS: 1.27%, COR: 80.88%, WER: 20.38%

Results for test_clean (lowest validation error):
SUB: 10.48%, DEL: 0.66%, INS: 0.64%, COR: 88.86%, WER: 11.78%
Results for test_other (lowest validation error):
SUB: 18.12%, DEL: 1.98%, INS: 1.16%, COR: 79.90%, WER: 21.25%