Namespace(seed=123, train_json='data/train_10h.json', val_json='data/devsubset.json', test_clean_json='data/test_clean.json', test_other_json='data/test_other.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=3, lr=0.0001, vocab='data/vocab.txt', use_fbank=False, model='wav2vec2', report_interval=50, num_epochs=10)
Total number of model parameters is 94394782
EPOCH 1:
  batch 50 loss: 7.9695374870300295
  batch 100 loss: 6.328239507675171
  batch 150 loss: 4.857379875183105
  batch 200 loss: 3.8755268716812132
  batch 250 loss: 3.450623836517334
  batch 300 loss: 3.2367423391342163
  batch 350 loss: 3.138535485267639
  batch 400 loss: 3.085381770133972
  batch 450 loss: 3.0179546689987182
  batch 500 loss: 2.9841813325881956
  batch 550 loss: 3.011878023147583
  batch 600 loss: 2.9651906538009642
  batch 650 loss: 2.9382493352890013
LOSS train 2.93825 valid 3.06676, valid WER 100.00%
EPOCH 2:
  batch 50 loss: 2.945429310798645
  batch 100 loss: 2.936913237571716
  batch 150 loss: 2.930266971588135
  batch 200 loss: 2.9065672540664673
  batch 250 loss: 2.910235733985901
  batch 300 loss: 2.899728741645813
  batch 350 loss: 2.894079174995422
  batch 400 loss: 2.8955716037750245
  batch 450 loss: 2.8903535079956053
  batch 500 loss: 2.8909751176834106
  batch 550 loss: 2.8776935148239136
  batch 600 loss: 2.8658887767791748
  batch 650 loss: 2.880219588279724
LOSS train 2.88022 valid 2.98575, valid WER 100.00%
EPOCH 3:
  batch 50 loss: 2.848265986442566
  batch 100 loss: 2.8459274435043334
  batch 150 loss: 2.830494565963745
  batch 200 loss: 2.8121427202224734
  batch 250 loss: 2.8209753942489626
  batch 300 loss: 2.807626819610596
  batch 350 loss: 2.8127416372299194
  batch 400 loss: 2.7972339487075804
  batch 450 loss: 2.7583028411865236
  batch 500 loss: 2.758588070869446
  batch 550 loss: 2.74473198890686
  batch 600 loss: 2.7340035104751585
  batch 650 loss: 2.7125711965560915
LOSS train 2.71257 valid 2.83199, valid WER 100.00%
EPOCH 4:
  batch 50 loss: 2.3484199810028077
  batch 100 loss: 1.6291899919509887
  batch 150 loss: 1.079060229063034
  batch 200 loss: 0.8009128892421722
  batch 250 loss: 0.6978787493705749
  batch 300 loss: 0.6303389358520508
  batch 350 loss: 0.5692265701293945
  batch 400 loss: 0.5325426924228668
  batch 450 loss: 0.5253348952531814
  batch 500 loss: 0.4725571841001511
  batch 550 loss: 0.44304406762123105
  batch 600 loss: 0.4466689920425415
  batch 650 loss: 0.42573819518089295
LOSS train 0.42574 valid 0.51070, valid WER 40.85%
EPOCH 5:
  batch 50 loss: 0.3678445887565613
  batch 100 loss: 0.35137244552373886
  batch 150 loss: 0.35331601202487944
  batch 200 loss: 0.3249656906723976
  batch 250 loss: 0.3450696361064911
  batch 300 loss: 0.31924332469701766
  batch 350 loss: 0.33001548022031785
  batch 400 loss: 0.31864735871553423
  batch 450 loss: 0.30202981203794477
  batch 500 loss: 0.2988006031513214
  batch 550 loss: 0.2794951868057251
  batch 600 loss: 0.30749847292900084
  batch 650 loss: 0.31879180908203125
LOSS train 0.31879 valid 0.40459, valid WER 30.73%
EPOCH 6:
  batch 50 loss: 0.25188852071762086
  batch 100 loss: 0.23996057868003845
  batch 150 loss: 0.24459440767765045
  batch 200 loss: 0.22827880531549455
  batch 250 loss: 0.22575196385383606
  batch 300 loss: 0.23724634289741517
  batch 350 loss: 0.23613945811986922
  batch 400 loss: 0.2317843708395958
  batch 450 loss: 0.22644755572080613
  batch 500 loss: 0.22572885274887086
  batch 550 loss: 0.21449333652853966
  batch 600 loss: 0.219774908721447
  batch 650 loss: 0.21684859603643417
LOSS train 0.21685 valid 0.50397, valid WER 27.12%
EPOCH 7:
  batch 50 loss: 0.19473255231976508
  batch 100 loss: 0.18049127131700515
  batch 150 loss: 0.18707983702421188
  batch 200 loss: 0.19652226343750953
  batch 250 loss: 0.19538746863603593
  batch 300 loss: 0.18050562605261802
  batch 350 loss: 0.17954736366868018
  batch 400 loss: 0.1873921324312687
  batch 450 loss: 0.17419578090310098
  batch 500 loss: 0.18328196689486503
  batch 550 loss: 0.17079778552055358
  batch 600 loss: 0.17804094299674034
  batch 650 loss: 0.17118388071656226
LOSS train 0.17118 valid 0.34704, valid WER 24.13%
EPOCH 8:
  batch 50 loss: 0.14605195939540863
  batch 100 loss: 0.1605916415154934
  batch 150 loss: 0.1551684196293354
  batch 200 loss: 0.14795585364103317
  batch 250 loss: 0.14545302495360374
  batch 300 loss: 0.1703287310898304
  batch 350 loss: 0.15830634504556657
  batch 400 loss: 0.1584479370713234
  batch 450 loss: 0.13740965351462364
  batch 500 loss: 0.13735144719481468
  batch 550 loss: 0.15126045867800714
  batch 600 loss: 0.1370403927564621
  batch 650 loss: 0.15658627390861513
LOSS train 0.15659 valid 0.55615, valid WER 22.80%
EPOCH 9:
  batch 50 loss: 0.12978193074464797
  batch 100 loss: 0.12867717929184436
  batch 150 loss: 0.12545331463217735
  batch 200 loss: 0.1258812003582716
  batch 250 loss: 0.12309647351503372
  batch 300 loss: 0.12995343573391438
  batch 350 loss: 0.12082914844155311
  batch 400 loss: 0.12165289387106895
  batch 450 loss: 0.12776596531271933
  batch 500 loss: 0.12774372190237046
  batch 550 loss: 0.12516713730990886
  batch 600 loss: 0.13537087947130202
  batch 650 loss: 0.13553367376327516
LOSS train 0.13553 valid 0.58294, valid WER 21.54%
EPOCH 10:
  batch 50 loss: 0.1277208635210991
  batch 100 loss: 0.12613211050629616
  batch 150 loss: 0.10436284810304641
  batch 200 loss: 0.10739933408796787
  batch 250 loss: 0.11745026707649231
  batch 300 loss: 0.10886176533997059
  batch 350 loss: 0.11040421545505524
  batch 400 loss: 0.11807117968797684
  batch 450 loss: 0.1100051935762167
  batch 500 loss: 0.1032810216397047
  batch 550 loss: 0.11374821037054061
  batch 600 loss: 0.11834075421094895
  batch 650 loss: 0.11892299257218837
LOSS train 0.11892 valid 0.45390, valid WER 21.29%
Training finished in 23.0 minutes.
Model saved to checkpoints/20230307_171740/model_7
Loading model from checkpoints/20230307_171740/model_7
Results for test_clean:
SUB: 20.17%, DEL: 0.76%, INS: 1.22%, COR: 79.07%, WER: 22.15%
Results for test_other:
SUB: 25.31%, DEL: 1.57%, INS: 1.69%, COR: 73.12%, WER: 28.57%
