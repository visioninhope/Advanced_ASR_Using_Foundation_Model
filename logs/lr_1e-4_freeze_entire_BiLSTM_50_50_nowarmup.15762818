Namespace(seed=123, train_json='data/train_10h.json', val_json='data/devsubset.json', test_clean_json='data/test_clean.json', test_other_json='data/test_other.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=3, lr=0.0001, vocab='data/vocab.txt', use_fbank=False, model='wav2vec2', report_interval=50, num_epochs=10)
Total number of model parameters is 95298974
EPOCH 1:
  batch 50 loss: 6.835052909851075
  batch 100 loss: 3.2938725566864013
  batch 150 loss: 2.9814226055145263
  batch 200 loss: 2.9367742586135863
  batch 250 loss: 2.928140916824341
  batch 300 loss: 2.9145922803878785
  batch 350 loss: 2.911637020111084
  batch 400 loss: 2.9150764465332033
  batch 450 loss: 2.8815743589401244
  batch 500 loss: 2.8819390106201173
  batch 550 loss: 2.8845036220550537
  batch 600 loss: 2.8828088426589966
  batch 650 loss: 2.866969656944275
LOSS train 2.86697 valid 2.98617, valid WER 100.00%
EPOCH 2:
  batch 50 loss: 2.879852590560913
  batch 100 loss: 2.8750666999816894
  batch 150 loss: 2.868744626045227
  batch 200 loss: 2.8692204427719115
  batch 250 loss: 2.865967450141907
  batch 300 loss: 2.8659828758239745
  batch 350 loss: 2.8643561887741087
  batch 400 loss: 2.8622700548171998
  batch 450 loss: 2.8674450159072875
  batch 500 loss: 2.8658631896972655
  batch 550 loss: 2.8636709213256837
  batch 600 loss: 2.859102110862732
  batch 650 loss: 2.858735237121582
LOSS train 2.85874 valid 2.98354, valid WER 100.00%
EPOCH 3:
  batch 50 loss: 2.8565506172180175
  batch 100 loss: 2.8506826639175413
  batch 150 loss: 2.8440748691558837
  batch 200 loss: 2.8340298986434935
  batch 250 loss: 2.842236762046814
  batch 300 loss: 2.8372674798965454
  batch 350 loss: 2.8323717451095582
  batch 400 loss: 2.8198627424240112
  batch 450 loss: 2.7891993808746336
  batch 500 loss: 2.7829137897491454
  batch 550 loss: 2.761284213066101
  batch 600 loss: 2.74097975730896
  batch 650 loss: 2.7107108640670776
LOSS train 2.71071 valid 2.79469, valid WER 99.90%
EPOCH 4:
  batch 50 loss: 2.6623965883255005
  batch 100 loss: 2.637450532913208
  batch 150 loss: 2.598888702392578
  batch 200 loss: 2.569772138595581
  batch 250 loss: 2.5379406309127805
  batch 300 loss: 2.508418598175049
  batch 350 loss: 2.4701997566223146
  batch 400 loss: 2.443392343521118
  batch 450 loss: 2.419059844017029
  batch 500 loss: 2.408154397010803
  batch 550 loss: 2.365418028831482
  batch 600 loss: 2.3535723161697386
  batch 650 loss: 2.3229272174835205
LOSS train 2.32293 valid 2.47510, valid WER 99.46%
EPOCH 5:
  batch 50 loss: 2.2716144847869875
  batch 100 loss: 2.2588400506973265
  batch 150 loss: 2.223737564086914
  batch 200 loss: 2.22097629070282
  batch 250 loss: 2.2090996980667112
  batch 300 loss: 2.181405954360962
  batch 350 loss: 2.18101309299469
  batch 400 loss: 2.151342921257019
  batch 450 loss: 2.138313753604889
  batch 500 loss: 2.117405149936676
  batch 550 loss: 2.117724506855011
  batch 600 loss: 2.1088301229476927
  batch 650 loss: 2.0957064032554626
LOSS train 2.09571 valid 2.31596, valid WER 99.36%
EPOCH 6:
  batch 50 loss: 2.0673009037971495
  batch 100 loss: 2.044922797679901
  batch 150 loss: 2.0513963150978087
  batch 200 loss: 2.036397202014923
  batch 250 loss: 2.0444538593292236
  batch 300 loss: 2.0383752298355104
  batch 350 loss: 2.0152181434631347
  batch 400 loss: 2.0048652839660646
  batch 450 loss: 1.9959862303733826
  batch 500 loss: 1.999479022026062
  batch 550 loss: 1.9987427616119384
  batch 600 loss: 1.9941652870178224
  batch 650 loss: 1.9768620252609252
LOSS train 1.97686 valid 2.22526, valid WER 99.09%
EPOCH 7:
  batch 50 loss: 1.9745195770263673
  batch 100 loss: 1.9641278958320618
  batch 150 loss: 1.9602943515777589
  batch 200 loss: 1.9601156210899353
  batch 250 loss: 1.936590142250061
  batch 300 loss: 1.9526974439620972
  batch 350 loss: 1.942079360485077
  batch 400 loss: 1.937185332775116
  batch 450 loss: 1.931356828212738
  batch 500 loss: 1.9347298622131348
  batch 550 loss: 1.917227747440338
  batch 600 loss: 1.9276483154296875
  batch 650 loss: 1.9191345763206482
LOSS train 1.91913 valid 2.16268, valid WER 99.09%
EPOCH 8:
  batch 50 loss: 1.9028404307365419
  batch 100 loss: 1.9037689280509948
  batch 150 loss: 1.9112528157234192
  batch 200 loss: 1.9080588388442994
  batch 250 loss: 1.9156026673316955
  batch 300 loss: 1.907454354763031
  batch 350 loss: 1.8695672464370727
  batch 400 loss: 1.8771361470222474
  batch 450 loss: 1.8988721084594726
  batch 500 loss: 1.8840772008895874
  batch 550 loss: 1.8976778078079224
  batch 600 loss: 1.903683729171753
  batch 650 loss: 1.8976666164398193
LOSS train 1.89767 valid 2.13591, valid WER 98.97%
EPOCH 9:
  batch 50 loss: 1.8930171418190003
  batch 100 loss: 1.8977463221549988
  batch 150 loss: 1.8754411792755128
  batch 200 loss: 1.8682174253463746
  batch 250 loss: 1.87200359582901
  batch 300 loss: 1.8804855465888977
  batch 350 loss: 1.882501630783081
  batch 400 loss: 1.8842911648750305
  batch 450 loss: 1.8697478413581847
  batch 500 loss: 1.84501398563385
  batch 550 loss: 1.8789280319213868
  batch 600 loss: 1.86506986618042
  batch 650 loss: 1.885987277030945
LOSS train 1.88599 valid 2.12267, valid WER 98.97%
EPOCH 10:
  batch 50 loss: 1.8624502277374269
  batch 100 loss: 1.8725655746459962
  batch 150 loss: 1.8459151005744934
  batch 200 loss: 1.8667110085487366
  batch 250 loss: 1.8743315267562866
  batch 300 loss: 1.8491263175010682
  batch 350 loss: 1.8681189846992492
  batch 400 loss: 1.8590081095695496
  batch 450 loss: 1.8630437469482422
  batch 500 loss: 1.865855643749237
  batch 550 loss: 1.8462727284431457
  batch 600 loss: 1.872661612033844
  batch 650 loss: 1.8802204632759094
LOSS train 1.88022 valid 2.11281, valid WER 98.95%
Training finished in 10.0 minutes.
Model saved to checkpoints/20230308_222619/model_10
Loading model from checkpoints/20230308_222619/model_10
Results for test_clean:
SUB: 51.72%, DEL: 47.07%, INS: 0.05%, COR: 1.21%, WER: 98.84%
Results for test_other:
SUB: 53.20%, DEL: 45.73%, INS: 0.09%, COR: 1.07%, WER: 99.02%
