Namespace(seed=123, train_json='data/train_10h.json', val_json='data/devsubset.json', test_clean_json='data/test_clean.json', test_other_json='data/test_other.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=3, lr=0.0001, vocab='data/vocab.txt', use_fbank=False, model='wav2vec2', report_interval=50, num_epochs=10)
Total number of model parameters is 94394782
EPOCH 1:
  batch 50 loss: 8.019715461730957
  batch 100 loss: 6.476632976531983
  batch 150 loss: 5.02938910484314
  batch 200 loss: 3.9644009017944337
  batch 250 loss: 3.4476074266433714
  batch 300 loss: 3.2075687885284423
  batch 350 loss: 3.1113515996932986
  batch 400 loss: 3.06624792098999
  batch 450 loss: 3.0042324113845824
  batch 500 loss: 2.9678339910507203
  batch 550 loss: 2.992217164039612
  batch 600 loss: 2.9526310777664184
  batch 650 loss: 2.934593286514282
LOSS train 2.93459 valid 3.06004, valid WER 100.00%
EPOCH 2:
  batch 50 loss: 2.9406068563461303
  batch 100 loss: 2.9272211742401124
  batch 150 loss: 2.920257959365845
  batch 200 loss: 2.914140753746033
  batch 250 loss: 2.9076239919662474
  batch 300 loss: 2.8980268096923827
  batch 350 loss: 2.8899356746673583
  batch 400 loss: 2.8889574432373046
  batch 450 loss: 2.88278160572052
  batch 500 loss: 2.882301716804504
  batch 550 loss: 2.874342975616455
  batch 600 loss: 2.8622151708602903
  batch 650 loss: 2.873998260498047
LOSS train 2.87400 valid 2.97940, valid WER 100.00%
EPOCH 3:
  batch 50 loss: 2.844921226501465
  batch 100 loss: 2.838820948600769
  batch 150 loss: 2.823537964820862
  batch 200 loss: 2.804188594818115
  batch 250 loss: 2.8117770051956175
  batch 300 loss: 2.8024078607559204
  batch 350 loss: 2.804015250205994
  batch 400 loss: 2.787614417076111
  batch 450 loss: 2.7493422269821166
  batch 500 loss: 2.7505954837799074
  batch 550 loss: 2.7371364784240724
  batch 600 loss: 2.7255601024627687
  batch 650 loss: 2.7033267402648926
LOSS train 2.70333 valid 2.82024, valid WER 100.00%
EPOCH 4:
  batch 50 loss: 2.671287565231323
  batch 100 loss: 2.6641124629974366
  batch 150 loss: 2.645158576965332
  batch 200 loss: 2.625831055641174
  batch 250 loss: 2.619123320579529
  batch 300 loss: 2.597229743003845
  batch 350 loss: 2.5777739763259886
  batch 400 loss: 2.566353340148926
  batch 450 loss: 2.5568745803833006
  batch 500 loss: 2.548681797981262
  batch 550 loss: 2.525880813598633
  batch 600 loss: 2.5204878568649294
  batch 650 loss: 2.5036341190338134
LOSS train 2.50363 valid 2.65837, valid WER 99.90%
EPOCH 5:
  batch 50 loss: 2.4667685985565186
  batch 100 loss: 2.4635165452957155
  batch 150 loss: 2.4344669246673583
  batch 200 loss: 2.4394635820388793
  batch 250 loss: 2.4380699682235716
  batch 300 loss: 2.4064790487289427
  batch 350 loss: 2.408351197242737
  batch 400 loss: 2.3894423913955687
  batch 450 loss: 2.3811605167388916
  batch 500 loss: 2.367575807571411
  batch 550 loss: 2.367663850784302
  batch 600 loss: 2.3582073211669923
  batch 650 loss: 2.3519477319717406
LOSS train 2.35195 valid 2.55362, valid WER 99.81%
EPOCH 6:
  batch 50 loss: 2.3289908933639527
  batch 100 loss: 2.301618857383728
  batch 150 loss: 2.31632408618927
  batch 200 loss: 2.3012611436843873
  batch 250 loss: 2.3043306016922
  batch 300 loss: 2.298896017074585
  batch 350 loss: 2.2850308227539062
  batch 400 loss: 2.275691065788269
  batch 450 loss: 2.2613572216033937
  batch 500 loss: 2.2700821495056154
  batch 550 loss: 2.2679817056655884
  batch 600 loss: 2.254504027366638
  batch 650 loss: 2.239116554260254
LOSS train 2.23912 valid 2.49145, valid WER 99.67%
EPOCH 7:
  batch 50 loss: 2.23748863697052
  batch 100 loss: 2.2337080144882204
  batch 150 loss: 2.2289313888549804
  batch 200 loss: 2.2273653841018675
  batch 250 loss: 2.2018988609313963
  batch 300 loss: 2.2208261394500735
  batch 350 loss: 2.2099260330200194
  batch 400 loss: 2.203279571533203
  batch 450 loss: 2.2028868103027346
  batch 500 loss: 2.2043072271347044
  batch 550 loss: 2.1791232776641847
  batch 600 loss: 2.1894659948349
  batch 650 loss: 2.1860927152633667
LOSS train 2.18609 valid 2.45139, valid WER 99.55%
EPOCH 8:
  batch 50 loss: 2.167688250541687
  batch 100 loss: 2.1731649255752563
  batch 150 loss: 2.1801098871231077
  batch 200 loss: 2.1763153982162478
  batch 250 loss: 2.177518754005432
  batch 300 loss: 2.1776019859313966
  batch 350 loss: 2.1400619029998778
  batch 400 loss: 2.142886371612549
  batch 450 loss: 2.161173982620239
  batch 500 loss: 2.152226324081421
  batch 550 loss: 2.155964114665985
  batch 600 loss: 2.165873017311096
  batch 650 loss: 2.159931468963623
LOSS train 2.15993 valid 2.42872, valid WER 99.40%
EPOCH 9:
  batch 50 loss: 2.152682356834412
  batch 100 loss: 2.155224041938782
  batch 150 loss: 2.135262315273285
  batch 200 loss: 2.1346180725097654
  batch 250 loss: 2.135192320346832
  batch 300 loss: 2.1430391550064085
  batch 350 loss: 2.147803750038147
  batch 400 loss: 2.1433203077316283
  batch 450 loss: 2.133565454483032
  batch 500 loss: 2.1067125463485716
  batch 550 loss: 2.137429404258728
  batch 600 loss: 2.1210885047912598
  batch 650 loss: 2.1507209658622743
LOSS train 2.15072 valid 2.41614, valid WER 99.42%
EPOCH 10:
  batch 50 loss: 2.1173165941238405
  batch 100 loss: 2.129934265613556
  batch 150 loss: 2.104758770465851
  batch 200 loss: 2.1286671423912047
  batch 250 loss: 2.138670859336853
  batch 300 loss: 2.1160759258270265
  batch 350 loss: 2.128056001663208
  batch 400 loss: 2.1219399976730347
  batch 450 loss: 2.1224961185455324
  batch 500 loss: 2.1208620619773866
  batch 550 loss: 2.1059657669067384
  batch 600 loss: 2.1339637899398802
  batch 650 loss: 2.143436994552612
LOSS train 2.14344 valid 2.41231, valid WER 99.42%
Training finished in 11.0 minutes.
Model saved to checkpoints/20230307_180217/model_10
Loading model from checkpoints/20230307_180217/model_10
Results for test_clean:
SUB: 33.96%, DEL: 65.68%, INS: 0.01%, COR: 0.36%, WER: 99.64%
Results for test_other:
SUB: 36.19%, DEL: 63.34%, INS: 0.01%, COR: 0.47%, WER: 99.54%
