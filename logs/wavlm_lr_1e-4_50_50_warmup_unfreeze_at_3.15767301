Namespace(seed=123, train_json='data/train_10h.json', val_json='data/devsubset.json', test_clean_json='data/test_clean.json', test_other_json='data/test_other.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=3, lr=0.0001, vocab='data/vocab.txt', use_fbank=False, model='wavlm', report_interval=50, num_epochs=10)
Total number of model parameters is 94405006
EPOCH 1:
  batch 50 loss: 8.60195673942566
  batch 100 loss: 8.55581337928772
  batch 150 loss: 8.431638221740723
  batch 200 loss: 8.318241739273072
  batch 250 loss: 8.25171109199524
  batch 300 loss: 8.003068914413452
  batch 350 loss: 7.566688928604126
  batch 400 loss: 7.345321388244629
  batch 450 loss: 6.790824718475342
  batch 500 loss: 6.296363897323609
  batch 550 loss: 5.927465476989746
  batch 600 loss: 5.248949699401855
  batch 650 loss: 4.652020559310913
LOSS train 4.65202 valid 4.39667, valid WER 100.00%
EPOCH 2:
  batch 50 loss: 4.234568829536438
  batch 100 loss: 4.0075352764129635
  batch 150 loss: 3.9444338703155517
  batch 200 loss: 3.996017966270447
  batch 250 loss: 3.6694088411331176
  batch 300 loss: 3.408070278167725
  batch 350 loss: 3.299844741821289
  batch 400 loss: 3.231920003890991
  batch 450 loss: 3.34773220539093
  batch 500 loss: 2.9608357095718385
  batch 550 loss: 3.3938992071151732
  batch 600 loss: 2.9494212579727175
  batch 650 loss: 3.0625877380371094
LOSS train 3.06259 valid 2.74933, valid WER 100.00%
EPOCH 3:
  batch 50 loss: 2.5523884391784666
  batch 100 loss: 2.708103723526001
  batch 150 loss: 2.4536631393432615
  batch 200 loss: 2.433207812309265
  batch 250 loss: 2.498857274055481
  batch 300 loss: 2.3044858503341676
  batch 350 loss: 2.172413363456726
  batch 400 loss: 2.193556995391846
  batch 450 loss: 2.082724828720093
  batch 500 loss: 2.133820333480835
  batch 550 loss: 2.0151093888282774
  batch 600 loss: 1.991701099872589
  batch 650 loss: 1.915674970149994
LOSS train 1.91567 valid 1.83771, valid WER 98.35%
EPOCH 4:
  batch 50 loss: 1.3507056534290314
  batch 100 loss: 0.9093343043327331
  batch 150 loss: 0.6281106907129288
  batch 200 loss: 0.6210134690999984
  batch 250 loss: 0.5410401743650436
  batch 300 loss: 0.6841257321834564
  batch 350 loss: 0.4740064924955368
  batch 400 loss: 0.4772937428951263
  batch 450 loss: 0.4120001009106636
  batch 500 loss: 0.37432127267122267
  batch 550 loss: 0.3728440845012665
  batch 600 loss: 0.34427146464586256
  batch 650 loss: 0.32165006071329116
LOSS train 0.32165 valid 0.36157, valid WER 27.99%
EPOCH 5:
  batch 50 loss: 0.3292179521918297
  batch 100 loss: 0.2519125360250473
  batch 150 loss: 0.25388911440968515
  batch 200 loss: 0.26713111221790314
  batch 250 loss: 0.2134999690949917
  batch 300 loss: 0.22317505538463592
  batch 350 loss: 0.2271617005765438
  batch 400 loss: 0.18428415283560753
  batch 450 loss: 0.17269125029444696
  batch 500 loss: 0.18873685762286185
  batch 550 loss: 0.16063796520233153
  batch 600 loss: 0.24036383822560312
  batch 650 loss: 0.2173736274242401
LOSS train 0.21737 valid 0.30646, valid WER 21.31%
EPOCH 6:
  batch 50 loss: 0.18059029847383498
  batch 100 loss: 0.17972052291035653
  batch 150 loss: 0.2773461410403252
  batch 200 loss: 0.14065834015607834
  batch 250 loss: 0.1365901666879654
  batch 300 loss: 0.17680195964872836
  batch 350 loss: 0.21899547100067138
  batch 400 loss: 0.16524583280086516
  batch 450 loss: 0.18002322115004063
  batch 500 loss: 0.13213502623140813
  batch 550 loss: 0.13676120087504387
  batch 600 loss: 0.15821705006062983
  batch 650 loss: 0.1461654318124056
LOSS train 0.14617 valid 0.28063, valid WER 16.92%
EPOCH 7:
  batch 50 loss: 0.10646082874387502
  batch 100 loss: 0.12474616013467311
  batch 150 loss: 0.11974507719278335
  batch 200 loss: 0.12704888343811035
  batch 250 loss: 0.09824918337166309
  batch 300 loss: 0.10646654304116965
  batch 350 loss: 0.12843395352363587
  batch 400 loss: 0.11794811449944972
  batch 450 loss: 0.10844936370849609
  batch 500 loss: 0.1159673598781228
  batch 550 loss: 0.13047995947301388
  batch 600 loss: 0.1373983046412468
  batch 650 loss: 0.09707492865622043
LOSS train 0.09707 valid 0.31836, valid WER 14.94%
EPOCH 8:
  batch 50 loss: 0.08721345329657197
  batch 100 loss: 0.11523825541138649
  batch 150 loss: 0.09849720779806376
  batch 200 loss: 0.10123102720826864
  batch 250 loss: 0.17590919382870196
  batch 300 loss: 0.09341142028570175
  batch 350 loss: 0.10779355481266975
  batch 400 loss: 0.13265502218157052
  batch 450 loss: 0.0769547901302576
  batch 500 loss: 0.07196738198399544
  batch 550 loss: 0.07350997557863594
  batch 600 loss: 0.07099037088453769
  batch 650 loss: 0.08423275329172611
LOSS train 0.08423 valid 0.30064, valid WER 14.36%
EPOCH 9:
  batch 50 loss: 0.07907652620226145
  batch 100 loss: 0.08789739003404975
  batch 150 loss: 0.0612733505666256
  batch 200 loss: 0.08167044010013341
  batch 250 loss: 0.07012783234007657
  batch 300 loss: 0.07522385116666555
  batch 350 loss: 0.07895600281655789
  batch 400 loss: 0.08538686826825143
  batch 450 loss: 0.053799553886055944
  batch 500 loss: 0.07409735854715109
  batch 550 loss: 0.06484833341091871
  batch 600 loss: 0.08634740589186549
  batch 650 loss: 0.09742005003616214
LOSS train 0.09742 valid 0.27169, valid WER 13.79%
EPOCH 10:
  batch 50 loss: 0.060919531378895046
  batch 100 loss: 0.07340260512195527
  batch 150 loss: 0.04463884062133729
  batch 200 loss: 0.04830253342166543
  batch 250 loss: 0.10318935284391045
  batch 300 loss: 0.0824578208103776
  batch 350 loss: 0.05683642629534006
  batch 400 loss: 0.07160099505446851
  batch 450 loss: 0.04960557942278683
  batch 500 loss: 0.05248581263236701
  batch 550 loss: 0.06593127315863967
  batch 600 loss: 0.06791985115036368
  batch 650 loss: 0.09335807695984841
LOSS train 0.09336 valid 0.25322, valid WER 13.05%
Training finished in 22.0 minutes.
Total number of unfreezed  model parameters is 89809550
Model saved to checkpoints/20230309_000630/model_10
Loading model from checkpoints/20230309_000630/model_10
Results for test_clean:
SUB: 9.18%, DEL: 0.59%, INS: 0.63%, COR: 90.23%, WER: 10.40%
Results for test_other:
SUB: 15.57%, DEL: 1.43%, INS: 1.22%, COR: 83.00%, WER: 18.22%
